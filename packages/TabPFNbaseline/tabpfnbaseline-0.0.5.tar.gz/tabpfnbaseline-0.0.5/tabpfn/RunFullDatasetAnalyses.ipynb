{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataFrame' from 'lightgbm.compat' (/Users/antanas/opt/anaconda3/envs/TabPFN/lib/python3.7/site-packages/lightgbm/compat.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0q/4mgwbnlj0tj34_6lyhzrbs5c0000gn/T/ipykernel_79109/3727572015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabpfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtabular_baselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TabPFN/lib/python3.7/site-packages/tabpfn/scripts/tabular_baselines.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1037\u001b[0m }  # 'normalize': [False],\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlightgbm_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_used\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TabPFN/lib/python3.7/site-packages/lightgbm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from .callback import (early_stopping, print_evaluation, record_evaluation,\n\u001b[1;32m     10\u001b[0m                        reset_parameter)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TabPFN/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from .compat import (PANDAS_INSTALLED, DataFrame, Series, is_dtype_sparse,\n\u001b[0m\u001b[1;32m     16\u001b[0m                      \u001b[0mDataTable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                      \u001b[0mdecode_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataFrame' from 'lightgbm.compat' (/Users/antanas/opt/anaconda3/envs/TabPFN/lib/python3.7/site-packages/lightgbm/compat.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabpfn.scripts import tabular_baselines\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datasets import load_openml_list, valid_dids_classification, test_dids_classification, open_cc_dids\n",
    "from tabpfn.scripts.tabular_baselines import *\n",
    "from tabpfn.scripts.tabular_evaluation import evaluate\n",
    "from tabpfn.scripts.tabular_metrics import calculate_score, make_ranks_and_wins_table, make_metric_matrix\n",
    "from tabpfn.scripts import tabular_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.notebook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 30\n",
      "Loading balance-scale 11 ..\n",
      "Loading mfeat-fourier 14 ..\n",
      "Loading breast-w 15 ..\n",
      "Loading mfeat-karhunen 16 ..\n",
      "Loading mfeat-morphological 18 ..\n",
      "Loading mfeat-zernike 22 ..\n",
      "Loading cmc 23 ..\n",
      "Loading credit-approval 29 ..\n",
      "Loading credit-g 31 ..\n",
      "Loading diabetes 37 ..\n",
      "Loading tic-tac-toe 50 ..\n",
      "Loading vehicle 54 ..\n",
      "Loading eucalyptus 188 ..\n",
      "Loading analcatdata_authorship 458 ..\n",
      "Loading analcatdata_dmft 469 ..\n",
      "Loading pc4 1049 ..\n",
      "Loading pc3 1050 ..\n",
      "Loading kc2 1063 ..\n",
      "Loading pc1 1068 ..\n",
      "Loading banknote-authentication 1462 ..\n",
      "Loading blood-transfusion-service-center 1464 ..\n",
      "Loading ilpd 1480 ..\n",
      "Loading qsar-biodeg 1494 ..\n",
      "Loading wdbc 1510 ..\n",
      "Loading cylinder-bands 6332 ..\n",
      "Loading dresses-sales 23381 ..\n",
      "Loading MiceProtein 40966 ..\n",
      "Loading car 40975 ..\n",
      "Loading steel-plates-fault 40982 ..\n",
      "Loading climate-model-simulation-crashes 40994 ..\n"
     ]
    }
   ],
   "source": [
    "cc_test_datasets_multiclass, cc_test_datasets_multiclass_df = load_openml_list(open_cc_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 10000, num_feats=100, return_capped=True) # max_num_classes=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cc_test_datasets_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(selector, task_type, suite='openml'):\n",
    "    if task_type == 'binary':\n",
    "        ds = valid_datasets_binary if selector == 'valid' else test_datasets_binary\n",
    "    else:\n",
    "        if suite == 'openml':\n",
    "            ds = valid_datasets_multiclass if selector == 'valid' else test_datasets_multiclass\n",
    "        elif suite == 'cc':\n",
    "            ds = valid_datasets_multiclass if selector == 'valid' else cc_test_datasets_multiclass\n",
    "            # print(f\"cc_test_datasets_multiclass is used\")\n",
    "        else:\n",
    "            raise Exception(\"Unknown suite\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_positions = [1000]\n",
    "max_features = 100\n",
    "bptt = 2000\n",
    "selector = 'test'\n",
    "base_path = os.path.join('.')\n",
    "overwrite=False\n",
    "# max_times = [0.5, 1, 15, 30, 60, 60*5, 60*15, 60*60]\n",
    "max_times = [0.5]\n",
    "metric_used = tabular_metrics.auc_metric\n",
    "# methods = ['transformer', 'logistic', 'gp', 'knn', 'catboost', 'xgb', 'autosklearn2', 'autogluon']\n",
    "methods = ['transformer','logistic']\n",
    "task_type = 'multiclass'\n",
    "\n",
    "# max_times = [0.5, 1, 5, 30, 60, 60*5, 60*15, 60*60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = 'cc'\n",
    "test_datasets = get_datasets('test',task_type, suite=suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict= {\n",
    "               'gp': gp_metric \n",
    "               , 'knn': knn_metric\n",
    "               , 'catboost': catboost_metric\n",
    "               , 'xgb': xgb_metric # kernel crashes\n",
    "               , 'transformer': transformer_metric # our model (1) - trained 1h\n",
    "               , 'i_transformer': transformer_metric # our model (2) - trained 12h\n",
    "               , 'i_transformer_2': transformer_metric # our model (3) - trained 12h\n",
    "               , 'i_transformer_3': transformer_metric # our model (4) - trained 12h\n",
    "               , 'tab_transformer': transformer_metric # original TabPFN - trained 10h\n",
    "               , 'logistic': logistic_metric\n",
    "               , 'autosklearn': autosklearn_metric\n",
    "               , 'autosklearn2': autosklearn2_metric\n",
    "               , 'autogluon': autogluon_metric # kernel crashes\n",
    "               , 'lgbm': lightgbm_metric # kernel crashes\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "def eval_method(task_type, method, dids, selector, eval_positions, max_time, metric_used, split_number, append_metric=True, fetch_only=False, verbose=False):\n",
    "    \n",
    "    dids = dids if type(dids) is list else [dids]\n",
    "    \n",
    "    for did in dids:\n",
    "\n",
    "        ds = get_datasets(selector, task_type, suite=suite)\n",
    "\n",
    "        ds = ds if did is None else ds[did:did+1]\n",
    "\n",
    "        clf = clf_dict[method]\n",
    "\n",
    "        time_string = '_time_'+str(max_time) if max_time else ''\n",
    "        metric_used_string = '_'+tabular_baselines.get_scoring_string(metric_used, usage='') if append_metric else ''\n",
    "\n",
    "        result = evaluate(datasets=ds\n",
    "                          , model=clf\n",
    "                          , method=method+time_string+metric_used_string\n",
    "                          , bptt=bptt, base_path='/cluster/scratch/amurelis'\n",
    "                          , eval_positions=eval_positions\n",
    "                          , device=device, max_splits=1\n",
    "                          , overwrite=overwrite\n",
    "                          , save=True\n",
    "                          , metric_used=metric_used\n",
    "                          , path_interfix=task_type\n",
    "                          , fetch_only=fetch_only\n",
    "                          , split_number=split_number\n",
    "                          , verbose=verbose\n",
    "                          , max_time=max_time)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Baseline Evaluation\n",
    "This section runs baselines and saves results locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "base_path = '/cluster/scratch/amurelis'\n",
    "!mkdir {base_path}/results\n",
    "!mkdir {base_path}/results/tabular/\n",
    "!mkdir {base_path}/results/tabular/multiclass/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from submitit import SlurmExecutor\n",
    "class BoschSlurmExecutor(SlurmExecutor):\n",
    "    def _make_submission_command(self, submission_file_path) -> List[str]:\n",
    "        return [\"sbatch\", str(submission_file_path)]\n",
    "\n",
    "ex = BoschSlurmExecutor(folder=\"./\")\n",
    "ex.update_parameters(time=1200\n",
    "                     , partition=\"baselines\"\n",
    "                     , mem_per_cpu=6000\n",
    "                    , nodes=1\n",
    "                    , cpus_per_task=1\n",
    "                    , ntasks_per_node=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%script echo skipping\n",
    "\n",
    "# RUN ALL METHODS, SPLITS AND DATASETS\n",
    "test_datasets = get_datasets('test', task_type, suite=suite)\n",
    "\n",
    "def submit_evaluation(dids, methods, max_times):\n",
    "    \n",
    "    mlp_jobs = [\n",
    "        ex.submit(eval_method, task, m, did, selector, eval_positions, max_time, metric_used, split_number)\n",
    "        #eval_method(task, m, did, selector, eval_positions, max_time)\n",
    "        for did in dids\n",
    "        for selector in ['test']\n",
    "        for metric_used in metric_used\n",
    "        for m in methods#['lightautoml', 'lightgbm', 'autogluon', 'logistic', 'gp', 'knn', 'catboost', 'xgb', 'autosklearn2'] #['knn', 'logistic', 'xgb', 'gp', 'autosklearn2', 'autogluon', 'catboost'] # 'knn',  'tabnet', 'logistic', 'xgb', 'autosklearn2'\n",
    "        for task in ['multiclass']\n",
    "        for max_time in max_times #[30, 60, 60*5, 60*15] # , 60, 60*15, 60*60\n",
    "        for split_number in [1, 2, 3, 4, 5]\n",
    "    ] \n",
    "    return mlp_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods = ['logistic']\n",
    "max_times = [60*5, 60*15]\n",
    "dids = range(30)\n",
    "overwrite=False\n",
    "suite = 'cc'\n",
    "\n",
    "test_datasets = get_datasets('test',task_type, suite=suite)\n",
    "methods = ['logistic']\n",
    "jobs = submit_evaluation(dids, methods, max_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jobs[9].stdout())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project_new",
   "language": "python",
   "name": "dl_project_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
