{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Infery Asynchronous Predict Example\n",
    "\n",
    "In this notebook we will demonstrate the usage of infery's TRT inferencer's `predict_async` method. We will showcase the performance boost you may gain by utilizing it to perform preprocessing, postprocessing and data transfer in parallel with inference on the GPU. Begin by importing infery and some basic libs for this notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ -INFO- Infery was successfully imported with 8 CPUS and 1 GPUS.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import infery\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union, Callable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import and model loading\n",
    "\n",
    "Now on to loading our TRT engine or pickle to the GPU. Notice the `concurrency` argument. This will determine how many async predictions we can launch at a time before our outputs may begin being overrun by following predictions. This point will become more clear later in this notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting framework...\n",
      "Detected framework for /home/naveassaf/Desktop/yolox_s.engine: FrameworkType.TENSORRT\n",
      "infery_manager -INFO- Loading model /home/naveassaf/Desktop/yolox_s.engine to the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n",
      "[TensorRT] WARNING: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infery_manager -INFO- Successfully loaded /home/naveassaf/Desktop/yolox_s.engine to the GPU.\n"
     ]
    }
   ],
   "source": [
    "# Prep some notebook globals\n",
    "WARMUP_ITERATIONS = 200\n",
    "BENCHMARK_ITERATIONS = 1000\n",
    "CONCURRENCY = 10\n",
    "ENGINE_PATH = 'ENGINE_PATH'\n",
    "\n",
    "# Load model\n",
    "model = infery.load(ENGINE_PATH, concurrency=CONCURRENCY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Asynchronous Predict Usage\n",
    "\n",
    "`predict_async` behaves similarly to the normal infery `predict` except that it returns an `AsyncExecutionHandle` object which you may either `completed` - to check if the execution has completed - or `get` to wait until the execution completes and get the result. For example:\n",
    "\n",
    "<br /><br />\n",
    "**Basic Usage**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE EXECUTION --- COMPLETED: False --- TIME: 1667405990426 [ms]\n",
      "AFTER EXECUTION  --- COMPLETED: True  --- TIME: 1667405990449 [ms]\n",
      "NORMAL == ASYNC: True\n"
     ]
    }
   ],
   "source": [
    "# Use infery's example_inputs to get a random np.ndarray input tensor\n",
    "test_input = model.example_inputs\n",
    "\n",
    "# Perform an asynchronous predict, demonstrating the get() and completed() functionality\n",
    "execution_handle = model.predict_async(test_input)\n",
    "print(f'BEFORE EXECUTION --- COMPLETED: {execution_handle.completed()} --- TIME: {round(time.time_ns()/1e6)} [ms]')\n",
    "asyc_predict_result = execution_handle.get()\n",
    "print(f'AFTER EXECUTION  --- COMPLETED: {execution_handle.completed()}  --- TIME: {round(time.time_ns()/1e6)} [ms]')\n",
    "\n",
    "# Ensure we received the same result as a normal predict. The index [0] here accesses the first output of the model.\n",
    "print(f'NORMAL == ASYNC: {(model.predict(test_input)[0] == asyc_predict_result[0]).all()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilizing Asynchronous Predict Within Your Application\n",
    "\n",
    "So far we have seen the boost achievable by utilizing `predict_async` to hide the data transfer latency of the model. In the cell below we will demonstrate how to hide preprocessing and postprocessing latencies by parallelizing inference on the GPU, pre/postprocessing on the CPU and data transfer between them. We begin by declaring our pre/postprocessing functions. Here these will just spin for `SLEEP_TIME` to simulate work on the CPU (more accurate than just sleeping).\n",
    "\n",
    "\n",
    "<br /><br />\n",
    "**Define Mock Pre/Postprocessing Callbacks**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SLEEP_TIME = 0.01\n",
    "\n",
    "def preprocess(x=None, **kwargs) -> Union[List[np.ndarray], Dict]:\n",
    "    # Mimic data fetching, augmentation, ...\n",
    "    current_time = time.time()\n",
    "    while time.time() < current_time + SLEEP_TIME: pass\n",
    "\n",
    "    return x\n",
    "\n",
    "def postprocess(x: List[np.ndarray], **kwargs):\n",
    "    # Render boxes, store results, ...\n",
    "    current_time = time.time()\n",
    "    while time.time() < current_time + SLEEP_TIME: pass\n",
    "\n",
    "    # Here we choose to return an output. This is not mandatory for postprocessing\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now iterate over a provided input in a sliding window, each time postprocessing `num_binding` inference results backwards, preprocessing the \"current\" input and sending it to the GPU by using `predict_async`. We first define a `predict_multi` function which receives multiple inputs and a `preprocessing_callback` and `postprocessing_callback` to perform on them.\n",
    "\n",
    "<br /><br />\n",
    "**Predict_Multi - Example Use of Asynchronous Predict To Speed Up Inference Over Multiple Inputs**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def predict_multi(multi_x: List, *, preprocessing_callback: Callable, postprocessing_callback: Callable) -> List[object]:\n",
    "    execution_handles = []\n",
    "    postprocessed_results = []\n",
    "    filled = False\n",
    "\n",
    "    for x in multi_x:\n",
    "        # Get inference outputs `num_bindings` back if there are enough enqueued\n",
    "        if len(execution_handles) >= CONCURRENCY or filled:\n",
    "            inference_result = execution_handles.pop(0).get()\n",
    "            postprocessed_results.append(postprocessing_callback(inference_result))\n",
    "            filled = True\n",
    "\n",
    "        # Preprocess and enqueue the current input\n",
    "        x = preprocessing_callback(x)\n",
    "        execution_handles.append(model.predict_async(x))\n",
    "\n",
    "    return postprocessed_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets check the performance difference.\n",
    "\n",
    "<br /><br />\n",
    "**Benchmarking Predict_Multi**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT MULTI TOOK: 32493.3111 [ms]\n",
      "PREDICT NORMAL TOOK: 50207.6232 [ms]\n"
     ]
    }
   ],
   "source": [
    "# Prep benchmarking resources and warm up the GPU\n",
    "test_input_list = [model.example_inputs] * BENCHMARK_ITERATIONS\n",
    "[model.predict(test_input) for _ in range(WARMUP_ITERATIONS)]\n",
    "\n",
    "# Benchmark predict_async's throughput.\n",
    "start = time.perf_counter()\n",
    "results = predict_multi(test_input_list, preprocessing_callback=preprocess, postprocessing_callback=postprocess)\n",
    "print(f'PREDICT MULTI TOOK: {round(time.perf_counter() - start, 7) * 1000} [ms]')\n",
    "\n",
    "# Roughly benchmark normal predict throughput\n",
    "start = time.perf_counter()\n",
    "results = []\n",
    "for x in test_input_list:\n",
    "        x = preprocess(x)\n",
    "        model.predict(x)\n",
    "        results.append(postprocess(x))\n",
    "print(f'PREDICT NORMAL TOOK: {round(time.perf_counter() - start, 7) * 1000} [ms]')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`predict_async` may also be used within a generator. If the input source is provided as an iterator, a blocking functionality may be utilized as a sort of async run loop that blocks either on the working GPU or a blocking read from the network for example:\n",
    "\n",
    "<br /><br />\n",
    "**Predict_Iter - Example Use of Asynchronous Predict To Speed Up Inference Over an Iterator**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def predict_iter(x_iterator, preprocessing_callback: Callable, postprocessing_callback: Callable):\n",
    "    execution_handles = []\n",
    "\n",
    "    for current_x in x_iterator():\n",
    "        # Get inference outputs `num_bindings` back if there are enough enqueued\n",
    "        if len(execution_handles) >= CONCURRENCY:\n",
    "            inference_result = execution_handles.pop(0).get()\n",
    "            postprocessing_callback(inference_result)\n",
    "\n",
    "        # Preprocess and enqueue the current input\n",
    "        current_x = preprocessing_callback(current_x)\n",
    "        execution_handles.append(model.predict_async(current_x))\n",
    "\n",
    "def data_generator():\n",
    "    # Transient problem - the `predict_iter` takes NUM_BINDINGS iterations to fill up, thus we perform an extra few iterations to get all results.\n",
    "    test_input_list = [model.example_inputs] * (BENCHMARK_ITERATIONS + CONCURRENCY)\n",
    "\n",
    "    for repetition in range(BENCHMARK_ITERATIONS + CONCURRENCY):\n",
    "        yield  test_input_list[repetition]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, lets run our generator. In many applications this call could iterate over the entire dataset or never return. Notice the result of the `postprocessing_callback` will not be used here - whatever necessary handling should be performed in the `postprocessing_callback`.\n",
    "\n",
    "<br /><br />\n",
    "**Benchmarking Predict_Iter**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT ITER TOOK: 32571.0046 [ms]\n",
      "PREDICT NORMAL TOOK: 50935.396 [ms]\n"
     ]
    }
   ],
   "source": [
    "# Prep benchmarking resources and warm up the GPU\n",
    "test_input_list = [model.example_inputs] * BENCHMARK_ITERATIONS\n",
    "[model.predict(test_input) for _ in range(WARMUP_ITERATIONS)]\n",
    "\n",
    "# Benchmark predict_async's throughput.\n",
    "start = time.perf_counter()\n",
    "results = predict_iter(data_generator, preprocessing_callback=preprocess, postprocessing_callback=postprocess)\n",
    "print(f'PREDICT ITER TOOK: {round(time.perf_counter() - start, 7) * 1000} [ms]')\n",
    "\n",
    "# Roughly benchmark normal predict throughput\n",
    "start = time.perf_counter()\n",
    "for x in test_input_list:\n",
    "        x = preprocess(x)\n",
    "        model.predict(x)\n",
    "        postprocess(x)\n",
    "print(f'PREDICT NORMAL TOOK: {round(time.perf_counter() - start, 7) * 1000} [ms]')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}