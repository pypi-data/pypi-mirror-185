{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, TransformerEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "import gpytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test batch (fast_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_features = 20 # setting this manually\n",
    "\n",
    "class PriorDataLoader(DataLoader):\n",
    "    pass\n",
    "    # init accepts num_steps as first argument\n",
    "\n",
    "    # has two attributes set on class or object level:\n",
    "    # num_features: int and\n",
    "    # num_outputs: int\n",
    "    # fuse_x_y: bool\n",
    "    # Optional: validate function that accepts a transformer model\n",
    "\n",
    "# tabpfn/utils.py\n",
    "def set_locals_in_self(locals):\n",
    "    \"\"\"\n",
    "    Call this function like `set_locals_in_self(locals())` to set all local variables as object variables.\n",
    "    Especially useful right at the beginning of `__init__`.\n",
    "    :param locals: `locals()`\n",
    "    \"\"\"\n",
    "    self = locals['self']\n",
    "    for var_name, val in locals.items():\n",
    "        if var_name != 'self': setattr(self, var_name, val)\n",
    "\n",
    "default_device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "\n",
    "# priors/utils.py\n",
    "def get_batch_to_dataloader(get_batch_method_):\n",
    "    class DL(PriorDataLoader):\n",
    "        get_batch_method = get_batch_method_\n",
    "\n",
    "        num_features = num_features\n",
    "\n",
    "        # Caution, you might need to set self.num_features manually if it is not part of the args.\n",
    "        def __init__(self, num_steps, **get_batch_kwargs):\n",
    "            set_locals_in_self(locals())\n",
    "\n",
    "            # The stuff outside the or is set as class attribute before instantiation.\n",
    "            self.num_features = get_batch_kwargs.get('num_features') or self.num_features\n",
    "            self.epoch_count = 0\n",
    "            #print('DataLoader.__dict__', self.__dict__)\n",
    "\n",
    "        @staticmethod\n",
    "        def gbm(*args, eval_pos_seq_len_sampler, **kwargs):\n",
    "            kwargs['single_eval_pos'], kwargs['seq_len'] = eval_pos_seq_len_sampler()\n",
    "            # Scales the batch size dynamically with the power of 'dynamic_batch_size'.\n",
    "            # A transformer with quadratic memory usage in the seq len would need a power of 2 to keep memory constant.\n",
    "            if 'dynamic_batch_size' in kwargs and kwargs['dynamic_batch_size'] > 0 and kwargs['dynamic_batch_size']:\n",
    "                kwargs['batch_size'] = kwargs['batch_size'] * math.floor(math.pow(kwargs['seq_len_maximum'], kwargs['dynamic_batch_size']) / math.pow(kwargs['seq_len'], kwargs['dynamic_batch_size']))\n",
    "            batch = get_batch_method_(*args, **kwargs)\n",
    "            x, y, target_y, style = batch if len(batch) == 4 else (batch[0], batch[1], batch[2], None)\n",
    "            return (style, x, y), target_y, kwargs['single_eval_pos']\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_steps\n",
    "\n",
    "        def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "        def __iter__(self):\n",
    "            assert hasattr(self, 'model'), \"Please assign model with `dl.model = ...` before training.\"\n",
    "            self.epoch_count += 1\n",
    "            return iter(self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count - 1, model=self.model) for _ in range(self.num_steps))\n",
    "\n",
    "    return DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from priors/fast_gp.py\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def get_model(x, y, hyperparameters):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1.e-9))\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    model.likelihood.noise = torch.ones_like(model.likelihood.noise) * hyperparameters[\"noise\"]\n",
    "    model.covar_module.outputscale = torch.ones_like(model.covar_module.outputscale) * hyperparameters[\"outputscale\"]\n",
    "    model.covar_module.base_kernel.lengthscale = torch.ones_like(model.covar_module.base_kernel.lengthscale) * \\\n",
    "                                                 hyperparameters[\"lengthscale\"]\n",
    "    return model, likelihood\n",
    "\n",
    "# manually setting num_features=20\n",
    "@torch.no_grad()\n",
    "def get_batch(batch_size, seq_len, num_features=20, device=default_device, hyperparameters=None,\n",
    "              equidistant_x=False, fix_x=None, **kwargs):\n",
    "    if isinstance(hyperparameters, (tuple, list)):\n",
    "        hyperparameters = {\"noise\": hyperparameters[0]\n",
    "            , \"outputscale\": hyperparameters[1]\n",
    "            , \"lengthscale\": hyperparameters[2]\n",
    "            , \"is_binary_classification\": hyperparameters[3]\n",
    "            # , \"num_features_used\": hyperparameters[4]\n",
    "            , \"normalize_by_used_features\": hyperparameters[5]\n",
    "            , \"order_y\": hyperparameters[6]\n",
    "            , \"sampling\": hyperparameters[7]\n",
    "                           }\n",
    "    elif hyperparameters is None:\n",
    "        hyperparameters = {\"noise\": .1, \"outputscale\": .1, \"lengthscale\": .1}\n",
    "\n",
    "    if 'verbose' in hyperparameters and hyperparameters['verbose']:\n",
    "        print({\"noise\": hyperparameters['noise'], \"outputscale\": hyperparameters['outputscale']\n",
    "                  , \"lengthscale\": hyperparameters['lengthscale'], 'batch_size': batch_size, 'sampling': hyperparameters['sampling']})\n",
    "\n",
    "    # hyperparameters = {k: hyperparameters[k]() if callable(hyperparameters[k]) else hyperparameters[k] for k in\n",
    "    #      hyperparameters.keys()}\n",
    "    assert not (equidistant_x and (fix_x is not None))\n",
    "\n",
    "    with gpytorch.settings.fast_computations(*hyperparameters.get('fast_computations', (True, True, True))):\n",
    "        if equidistant_x:\n",
    "            assert num_features == 1\n",
    "            x = torch.linspace(0, 1., seq_len).unsqueeze(0).repeat(batch_size, 1).unsqueeze(-1)\n",
    "        elif fix_x is not None:\n",
    "            assert fix_x.shape == (seq_len, num_features)\n",
    "            x = fix_x.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        else:\n",
    "            if hyperparameters.get('sampling','uniform') == 'uniform':\n",
    "                x = torch.rand(batch_size, seq_len, num_features, device=device)\n",
    "            else:\n",
    "                x = torch.randn(batch_size, seq_len, num_features, device=device)\n",
    "        model, likelihood = get_model(x, torch.Tensor(), hyperparameters)\n",
    "        model.to(device)\n",
    "        # trained_model = ExactGPModel(train_x, train_y, likelihood).cuda()\n",
    "        # trained_model.eval()\n",
    "        is_fitted = False\n",
    "        while not is_fitted:\n",
    "            try:\n",
    "                with gpytorch.settings.prior_mode(True):\n",
    "                    model, likelihood = get_model(x, torch.Tensor(), hyperparameters)\n",
    "                    model.to(device)\n",
    "\n",
    "                    d = model(x)\n",
    "                    d = likelihood(d)\n",
    "                    sample = d.sample().transpose(0, 1)\n",
    "                    is_fitted = True\n",
    "            except RuntimeError: # This can happen when torch.linalg.eigh fails. Restart with new init resolves this.\n",
    "                print('GP Fitting unsuccessful, retrying.. ')\n",
    "                print(x)\n",
    "                print(hyperparameters)\n",
    "\n",
    "    if bool(torch.any(torch.isnan(x)).detach().cpu().numpy()):\n",
    "        print({\"noise\": hyperparameters['noise'], \"outputscale\": hyperparameters['outputscale']\n",
    "                  , \"lengthscale\": hyperparameters['lengthscale'], 'batch_size': batch_size})\n",
    "\n",
    "    # TODO: Multi output\n",
    "    return x.transpose(0, 1), sample, sample  # x.shape = (T,B,H)\n",
    "\n",
    "DataLoader = get_batch_to_dataloader(get_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabpfn/train.py\n",
    "torch.manual_seed(1)\n",
    "steps_per_epoch = 100 # set to 10\n",
    "batch_size = 200 # set to 1000\n",
    "bptt=10 # default in function train(), not changed afterwards\n",
    "bptt_extra_samples=None # default in function train(), not changed afterwards\n",
    "single_eval_pos_gen=None # default in function train(), not changed afterwards\n",
    "extra_prior_kwargs_dict={} # default in function train(), not changed afterwards\n",
    "gpu_device='cuda:0' # default in function train(), not changed afterwards\n",
    "device = gpu_device if torch.cuda.is_available() else 'cpu:0'\n",
    "single_eval_pos_gen=None\n",
    "\n",
    "def eval_pos_seq_len_sampler():\n",
    "    single_eval_pos = single_eval_pos_gen()\n",
    "    if bptt_extra_samples:\n",
    "        return single_eval_pos, single_eval_pos + bptt_extra_samples\n",
    "    else:\n",
    "        return single_eval_pos, bptt\n",
    "\n",
    "priordataloader_class = DataLoader\n",
    "\n",
    "dl = priordataloader_class(num_steps=steps_per_epoch, batch_size=batch_size, eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, seq_len_maximum=bptt+(bptt_extra_samples if bptt_extra_samples else 0), device=device, **extra_prior_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "def get_uniform_single_eval_pos_sampler(max_len, min_len=0):\n",
    "    \"\"\"\n",
    "    Just sample any evaluation position with the same weight\n",
    "    :return: Sampler that can be fed to `train()` as `single_eval_pos_gen`.\n",
    "    \"\"\"\n",
    "    return lambda: random.choices(range(min_len, max_len))[0]\n",
    "\n",
    "get_sampler = get_uniform_single_eval_pos_sampler\n",
    "permutation_invariant_max_eval_pos = 100 # very random, had to set it to sth but don't know what this is\n",
    "\n",
    "single_eval_pos_gen = get_sampler(permutation_invariant_max_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gp = dl.get_test_batch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([10, 200, 20])\n",
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "# tuple: (style, x, y)\n",
    "print(dl.get_test_batch()[0][0]) # style: seems like it's None :(\n",
    "print(dl.get_test_batch()[0][1].shape) # x: seems like its a batch of 10 samples where each has 200 x vectors each with 20 features\n",
    "print(dl.get_test_batch()[0][2].shape) # y: seems like its a batch of 10 samples where each has 200 x vectors of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7474, 0.6250, 0.1107, 0.2828, 0.1912],\n",
      "        [0.7711, 0.6751, 0.0138, 0.4008, 0.6349],\n",
      "        [0.5934, 0.3755, 0.3774, 0.0090, 0.6477],\n",
      "        [0.1862, 0.3648, 0.1937, 0.8451, 0.6535]])\n",
      "tensor([ 0.9026,  0.1836, -0.0214,  0.3186])\n"
     ]
    }
   ],
   "source": [
    "print(dl.get_test_batch()[0][1][0,0:4,0:5]) # rows are vecs x1, x2, x3, x4\n",
    "print(dl.get_test_batch()[0][2][0,0:4]) # elements are values y1, y2, y3, y4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = data_gp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passed into train() in train.py\n",
    "emsize=512 #yes, same in the paper\n",
    "nhead=4 #yes, same in the paper\n",
    "nhid=2*emsize # #yes, same in the paper: 1024\n",
    "nlayers=6 # hmm, paper says 12\n",
    "\n",
    "# encoder = \n",
    "n_out = 1 # can be 2 or sth else\n",
    "ninp = emsize\n",
    "nhead = nhead\n",
    "nhid = 2*emsize\n",
    "nlayers = nlayers\n",
    "dropout=0.0\n",
    "style_encoder=None\n",
    "y_encoder=None\n",
    "pos_encoder=None\n",
    "decoder=None\n",
    "input_normalization=False\n",
    "init_method=None\n",
    "pre_norm=False\n",
    "activation='gelu'\n",
    "recompute_attn=False\n",
    "num_global_att_tokens=0\n",
    "full_attention=False\n",
    "all_layers_same_init=False\n",
    "efficient_eval_masking=True\n",
    "\n",
    "num_features = 20 # no default, depends on the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# full\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``.\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state): # not sure what it does\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        torch.manual_seed(1)\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False by default and is not changed in model=TransformerModel() in train.py\n",
    "            src_ = self.norm1(src)\n",
    "            #print(\"not run\")\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        if isinstance(src_mask, tuple): # NOT RUN - AssertionError \n",
    "            # global attention setup\n",
    "            assert not self.self_attn.batch_first # AssertionError when batch_first=True: not True = False  --> so batch_first must be False (and it is - default False is not changed in model=TransformerModel() in train.py)\n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            \n",
    "            # I think this is not run as we get AssertionError: default src_key_padding_mask=None is not changed\n",
    "            # so we actually do what's in else (elif also gets AssertionError fot the same reason)\n",
    "            \n",
    "            global_src_mask, trainset_src_mask, valset_src_mask = src_mask\n",
    "\n",
    "            num_global_tokens = global_src_mask.shape[0]\n",
    "            num_train_tokens = trainset_src_mask.shape[0]\n",
    "\n",
    "            global_tokens_src = src_[:num_global_tokens]\n",
    "            train_tokens_src = src_[num_global_tokens:num_global_tokens+num_train_tokens]\n",
    "            global_and_train_tokens_src = src_[:num_global_tokens+num_train_tokens]\n",
    "            eval_tokens_src = src_[num_global_tokens+num_train_tokens:]\n",
    "\n",
    "\n",
    "            attn = partial(checkpoint, self.self_attn) if self.recompute_attn else self.self_attn\n",
    "\n",
    "            global_tokens_src2 = attn(global_tokens_src, global_and_train_tokens_src, global_and_train_tokens_src, None, True, global_src_mask)[0]\n",
    "            train_tokens_src2 = attn(train_tokens_src, global_tokens_src, global_tokens_src, None, True, trainset_src_mask)[0]\n",
    "            eval_tokens_src2 = attn(eval_tokens_src, src_, src_,\n",
    "                                    None, True, valset_src_mask)[0]\n",
    "\n",
    "            src2 = torch.cat([global_tokens_src2, train_tokens_src2, eval_tokens_src2], dim=0)\n",
    "\n",
    "        elif isinstance(src_mask, int): # NOT RUN - AssertionError \n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            single_eval_position = src_mask\n",
    "            src_left = self.self_attn(src_[:single_eval_position], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src_right = self.self_attn(src_[single_eval_position:], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src2 = torch.cat([src_left, src_right], dim=0)\n",
    "        else: # this gets RUN \n",
    "            if self.recompute_attn: # recompute_attn=False by default, and is not changed in model=TransformerModel() in train.py)\n",
    "                src2 = checkpoint(self.self_attn, src_, src_, src_, src_key_padding_mask, True, src_mask)[0]\n",
    "            else: # so we actually do this part\n",
    "                src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False\n",
    "            src_ = self.norm2(src)\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src_))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm2(src)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.2793, 0.4031])\n",
      "torch.Size([10, 32, 512])\n",
      "tensor([ 0.5695, -1.0787,  0.1266], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_full = encoder_layer(src)\n",
    "\n",
    "print(src[0,0,0:3])\n",
    "print(out_full.shape)\n",
    "print(out_full[0,0,0:3]) # tensor([ 0.5695, -1.0787,  0.1266])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DelTransformerEncoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# commented out what's not run\n",
    "class DelTransformerEncoderLayer(Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout_ch = Dropout(dropout) # dropout -> dropout_ch\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        # multihead attention\n",
    "        src_ = src\n",
    "        src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # feed forward\n",
    "        src_ = src\n",
    "        src2 = self.linear2(self.dropout_ch(self.activation(self.linear1(src_)))) # dropout -> dropout_ch\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 0.5695, -1.0787,  0.1266], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer_del = DelTransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_deleted = encoder_layer_del(src)\n",
    "\n",
    "print(out_deleted.shape)\n",
    "print(out_deleted[0,0,0:3]) # tensor([ 0.5695, -1.0787,  0.1266])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer_creator = lambda: TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation=activation,\n",
    "                                                                pre_norm=pre_norm, recompute_attn=recompute_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDiffInit(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer_creator: a function generating objects of TransformerEncoderLayer class without args (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer_creator, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer_creator() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "transformer_encoder = TransformerEncoderDiffInit(encoder_layer_creator, nlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(self):\n",
    "    initrange = 1.\n",
    "    # if isinstance(self.encoder,EmbeddingEncoder):\n",
    "    #    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "    # self.decoder.bias.data.zero_()\n",
    "    # self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    if self.init_method is not None:\n",
    "        self.apply(self.init_method)\n",
    "    for layer in self.transformer_encoder.layers:\n",
    "        nn.init.zeros_(layer.linear2.weight)\n",
    "        nn.init.zeros_(layer.linear2.bias)\n",
    "        attns = layer.self_attn if isinstance(layer.self_attn, nn.ModuleList) else [layer.self_attn]\n",
    "        for attn in attns:\n",
    "            nn.init.zeros_(attn.out_proj.weight)\n",
    "            nn.init.zeros_(attn.out_proj.bias)\n",
    "\n",
    "def forward(self, src, src_mask=None, single_eval_pos=None):\n",
    "    assert isinstance(src, tuple), 'inputs (src) have to be given as (x,y) or (style,x,y) tuple'\n",
    "\n",
    "    if len(src) == 2: # (x,y) and no style\n",
    "        src = (None,) + src\n",
    "\n",
    "    style_src, x_src, y_src = src\n",
    "    x_src = self.encoder(x_src)\n",
    "    y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "    style_src = self.style_encoder(style_src).unsqueeze(0) if self.style_encoder else \\\n",
    "        torch.tensor([], device=x_src.device)\n",
    "    global_src = torch.tensor([], device=x_src.device) if self.global_att_embeddings is None else \\\n",
    "        self.global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)\n",
    "\n",
    "    if src_mask is not None: assert self.global_att_embeddings is None or isinstance(src_mask, tuple)\n",
    "    if src_mask is None: # this is RUN: default src_mask=None not changed it seems\n",
    "        if self.global_att_embeddings is None: # this is RUN: global_att_embeddings=None it seems\n",
    "            full_len = len(x_src) + len(style_src)\n",
    "            if self.full_attention:\n",
    "                src_mask = bool_mask_to_att_mask(torch.ones((full_len, full_len), dtype=torch.bool)).to(x_src.device)\n",
    "            elif self.efficient_eval_masking:\n",
    "                src_mask = single_eval_pos + len(style_src)\n",
    "            else:\n",
    "                src_mask = self.generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)\n",
    "        else:\n",
    "            src_mask_args = (self.global_att_embeddings.num_embeddings,\n",
    "                                len(x_src) + len(style_src),\n",
    "                                len(x_src) + len(style_src) - single_eval_pos)\n",
    "            src_mask = (self.generate_global_att_globaltokens_matrix(*src_mask_args).to(x_src.device),\n",
    "                        self.generate_global_att_trainset_matrix(*src_mask_args).to(x_src.device),\n",
    "                        self.generate_global_att_query_matrix(*src_mask_args).to(x_src.device))\n",
    "\n",
    "    train_x = x_src[:single_eval_pos] + y_src[:single_eval_pos]\n",
    "    src = torch.cat([global_src, style_src, train_x, x_src[single_eval_pos:]], 0)\n",
    "\n",
    "    if self.input_ln is not None:\n",
    "        src = self.input_ln(src)\n",
    "\n",
    "    if self.pos_encoder is not None:\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "    output = self.transformer_encoder(src, src_mask)\n",
    "    output = self.decoder(output)\n",
    "    return output[single_eval_pos+len(style_src)+(self.global_att_embeddings.num_embeddings if self.global_att_embeddings else 0):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
