Metadata-Version: 2.1
Name: infery-gpu
Version: 3.9.0
Summary: Deci Run-Time Engine
Home-page: https://github.com/Deci-AI/infery-examples
Author: Deci AI
Author-email: rnd@deci.ai
License: Deci Infery License
Keywords: Deci,AI,Inference,Deep Learning
Requires-Python: >=3.7.5
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic (==1.9.1)
Requires-Dist: psutil (==5.9.1)
Requires-Dist: requests (==2.28.1)
Requires-Dist: pandas (==1.3.4)
Requires-Dist: cryptography (==37.0.3)
Requires-Dist: numpy (==1.19.5)
Requires-Dist: typing-extensions (==4.4.0)
Requires-Dist: GPUtil (==1.4.0)
Requires-Dist: nvidia-pyindex (==1.0.8)
Requires-Dist: pycuda (==2021.1)
Provides-Extra: all
Requires-Dist: onnxruntime-gpu (==1.10.0) ; extra == 'all'
Requires-Dist: onnx (==1.12.0) ; extra == 'all'
Requires-Dist: tensorflow-gpu (==2.7.3) ; extra == 'all'
Requires-Dist: torch (==1.12.1) ; extra == 'all'
Requires-Dist: nvidia-tensorrt (==8.0.1.6) ; extra == 'all'
Requires-Dist: openvino (==2022.1) ; extra == 'all'
Provides-Extra: onnx
Requires-Dist: onnxruntime-gpu (==1.10.0) ; extra == 'onnx'
Requires-Dist: onnx (==1.12.0) ; extra == 'onnx'
Provides-Extra: openvino
Requires-Dist: openvino (==2022.1) ; extra == 'openvino'
Provides-Extra: openvino_21
Requires-Dist: openvino (==2021.4) ; extra == 'openvino_21'
Provides-Extra: openvino_22
Requires-Dist: openvino (==2022.1) ; extra == 'openvino_22'
Provides-Extra: tensorflow
Requires-Dist: tensorflow-gpu (==2.7.3) ; extra == 'tensorflow'
Provides-Extra: tensorrt
Requires-Dist: nvidia-tensorrt (==8.0.1.6) ; extra == 'tensorrt'
Requires-Dist: torch (==1.12.1) ; extra == 'tensorrt'
Provides-Extra: tensorrt_8.0
Requires-Dist: nvidia-tensorrt (==8.0.1.6) ; extra == 'tensorrt_8.0'
Requires-Dist: torch (==1.12.1) ; extra == 'tensorrt_8.0'
Provides-Extra: tensorrt_8.2
Requires-Dist: nvidia-tensorrt (==8.2.1.8) ; extra == 'tensorrt_8.2'
Requires-Dist: torch (==1.12.1) ; extra == 'tensorrt_8.2'
Provides-Extra: tensorrt_8.4
Requires-Dist: nvidia-tensorrt (==8.4.1.5) ; extra == 'tensorrt_8.4'
Requires-Dist: torch (==1.12.1) ; extra == 'tensorrt_8.4'
Provides-Extra: torch
Requires-Dist: torch (==1.12.1) ; extra == 'torch'

# Infery
`infery` is a runtime engine that simplifies inference, benchmark and profiling.<br>
`infery` supports all major deep learning frameworks, with a <b>unified and simple API</b>.

## Quickstart and Examples
<a href="https://github.com/Deci-AI/infery-examples">
    <img src="https://img.shields.io/badge/Public-Infery Examples-green">
</a>

### Installation Instructions
https://docs.deci.ai/docs/installing-infery-1

## Usage
![Infery Usage](https://deci.ai/wp-content/uploads/2021/06/Infery-Fig1.png)


#### 1. Load an example model
```python
>>> import infery, numpy as np
>>> model = infery.load(model_path='MyNewTask_1_0.onnx', framework_type='onnx', inference_hardware='gpu')
```

#### 2. Predict with a random numpy input

```python
>>> inputs = [np.random.random(shape).astype('float32') for shape in [[1, 243, 30, 40], [1, 172, 60, 80], [1, 102, 120, 160], [1, 64, 240, 320], [1, 153, 15, 20]]]
>>> model.predict(inputs)
[array([[[[-3.5920768, -3.5920792, -3.592102 , ..., -3.592099 ,
           -3.5920944, -3.5920882],
          [-3.592076 , -3.5919113, -3.592086 , ..., -3.5921211,
           -3.5921066, -3.5920937],
          [-3.592083 , -3.592073 , -3.5920823, ..., -3.5921297,
           -3.592109 , -3.5920937],
          ...,
          [-3.5920753, -3.5917826, -3.591444 , ..., -3.580754 ,
           -3.5816329, -3.582549 ],
          [-3.592073 , -3.5917459, -3.591257 , ..., -3.5817945,
           -3.5820704, -3.5835373],
          [-3.592073 , -3.5920737, -3.5920737, ..., -3.5853324,
           -3.5845006, -3.5856297]],
 
         [[-5.862198 , -5.8567815, -5.851764 , ..., -5.86396  ,
           -5.8639617, -5.865011 ],
          [-5.858771 , -5.8493323, -5.841462 , ..., -5.8617773,
           -5.8614554, -5.8633246],
          [-5.8560567, -5.844124 , -5.8351245, ..., -5.8598166,
           -5.859674 , -5.8624067],
          ...,
          [-5.8608136, -5.854358 , -5.8467784, ..., -5.8504496,
           -5.8563104, -5.8615303],
          [-5.86313  , -5.8587003, -5.8531966, ..., -5.8534794,
           -5.8581944, -5.8625536],
          [-5.865306 , -5.8623176, -5.8593984, ..., -5.8581495,
           -5.861572 , -5.865295 ]],
 
         [[-8.843734 , -8.840406 , -8.837172 , ..., -8.840413 ,
           -8.842026 , -8.843931 ],
          [-8.840792 , -8.836787 , -8.831037 , ..., -8.836103 ,
           -8.839954 , -8.842534 ],
          [-8.838855 , -8.833998 , -8.82706  , ..., -8.835106 ,
           -8.839087 , -8.841538 ],
          ...,
          [-8.842419 , -8.840865 , -8.838625 , ..., -8.83943  ,
           -8.843677 , -8.845087 ],
          [-8.844379 , -8.84402  , -8.843141 , ..., -8.84185  ,
           -8.844696 , -8.845202 ],
          [-8.844775 , -8.845572 , -8.845177 , ..., -8.843876 ,
           -8.8444395, -8.845926 ]]]], dtype=float32)]
```

#### 3. Benchmark the model on the current hardware
```python
>>> model.benchmark(batch_size=1)
-INFO- Benchmarking the model in batch size 1 and dimensions [(243, 30, 40), (172, 60, 80), (102, 120, 160), (64, 240, 320), (153, 15, 20)]... 
<ModelBenchmarks: {
    "batch_size": 1,
    "batch_inf_time": "6.57 ms",
    "batch_inf_time_variance": "0.02 ms",
    "model_memory_used": "1536.00 mb",
    "system_startpoint_memory_used": "1536.00 mb",
    "post_inference_memory_used": "1536.00 mb",
    "total_memory_size": "7982.00 mb",
    "throughput": "152.32 fps",
    "sample_inf_time": "6.57 ms",
    "include_io": true,
    "framework_type": "onnx",
    "framework_version": "1.10.0",
    "inference_hardware": "GPU",
    "date": "11:16:55__02-03-2022",
    "ctime": 1643879815,
    "h_to_d_mean": null,
    "d_to_h_mean": null,
    "h_to_d_variance": null,
    "d_to_h_variance": null
}>
```

## Documentation:
https://docs.deci.ai/docs/infery



