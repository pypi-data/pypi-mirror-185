{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([128, 20])\n",
      "output shape: torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30) # torch.nn.Linear(in_features, out_features)\n",
    "input = torch.randn(128, 20) # 128 vecs, each of length 20\n",
    "output = m(input)\n",
    "print(\"input shape:\", input.shape)\n",
    "print(\"output shape:\", output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding()\n",
    "\n",
    "* num_embeddings (int) – size of the dictionary of embeddings\n",
    "* embedding_dim (int) – the size of each embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 6])\n",
      "output shape: torch.Size([2, 6, 3])\n",
      "input: tensor([[1, 2, 4, 5, 7, 7],\n",
      "        [4, 3, 2, 9, 2, 2]])\n",
      "output: tensor([[[-1.6488, -0.6688, -0.1186],\n",
      "         [ 1.9160, -1.1260,  0.8322],\n",
      "         [-0.4320, -0.0880,  1.2710],\n",
      "         [ 0.0050,  0.7867, -1.0905],\n",
      "         [ 0.9532,  1.2178, -1.0842],\n",
      "         [ 0.9532,  1.2178, -1.0842]],\n",
      "\n",
      "        [[-0.4320, -0.0880,  1.2710],\n",
      "         [ 0.2345, -0.3219,  0.0278],\n",
      "         [ 1.9160, -1.1260,  0.8322],\n",
      "         [-0.8250, -0.5831, -1.6907],\n",
      "         [ 1.9160, -1.1260,  0.8322],\n",
      "         [ 1.9160, -1.1260,  0.8322]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10, 3) # torch.nn.Embedding(num_embeddings, embedding_dim, ...)\n",
    "# a batch of 2 samples of 6 indices each\n",
    "input = torch.LongTensor([[1,2,4,5,7,7],[4,3,2,9,2,2]])\n",
    "output = embedding(input)\n",
    "print(\"input shape:\", input.shape)\n",
    "print(\"output shape:\", output.size())\n",
    "print(\"input:\", input)\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[0, 2, 0, 5]])\n",
      "output: tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 1.3238,  1.3791,  0.9859],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4557,  0.3937, -0.7373]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "print(\"input:\", input)\n",
    "print(\"output:\", embedding(input))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "x = lambda a, b : a * b\n",
    "print(x(5, 6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_first = False\n",
    "src_key_padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not batch_first # AssertionError when batch_first=True: not True = False \n",
    "assert src_key_padding_mask is None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argparse.ArgumentParser()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a separate new file hello.py in folder my_files/\n",
    "\n",
    "\n",
    "[link: TowardsDataScience](https://towardsdatascience.com/a-simple-guide-to-command-line-arguments-with-argparse-6824c30ab1c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--name', type=str, required=True)\n",
    "# args = parser.parse_args()\n",
    "# print('Hello,', args.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in command line:\n",
    "\n",
    "1. cd my_files/\n",
    "2. python hello.py --name Ugne\n",
    "\n",
    "\n",
    "output:\n",
    "\n",
    "Hello, Ugne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I kind of tell python to execute file hello.py and also give the required argument --name: \"Ugne\"\n",
    "\n",
    "it executes the file so it checks if I gave the necessary argument and then does what the file tells: print('Hello,', args.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ._ _ dict _ _."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temp': 1}\n"
     ]
    }
   ],
   "source": [
    "def func():\n",
    "    pass\n",
    "\n",
    "func.temp = 1\n",
    "\n",
    "print(func.__dict__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we make an encoder block which consists of 6 encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "out = transformer_encoder(src)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(prior, \n",
    "#     criterion, \n",
    "#     encoder_generator, \n",
    "#     y_encoder_generator=y_encoder_generator, \n",
    "#     pos_encoder_generator=pos_encoder_generator, \n",
    "#     **args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def train(priordataloader_class, \n",
    "          criterion, \n",
    "          encoder_generator, \n",
    "          emsize=200, nhid=200, nlayers=6, nhead=2, dropout=0.0,\n",
    "          epochs=10, steps_per_epoch=100, batch_size=200, bptt=10, lr=None, weight_decay=0.0, warmup_epochs=10, input_normalization=False,\n",
    "          y_encoder_generator=None, pos_encoder_generator=None, decoder=None, extra_prior_kwargs_dict={}, scheduler=get_cosine_schedule_with_warmup,\n",
    "          load_weights_from_this_state_dict=None, validation_period=10, single_eval_pos_gen=None, bptt_extra_samples=None, gpu_device='cuda:0',\n",
    "          aggregate_k_gradients=1, verbose=True, style_encoder_generator=None, epoch_callback=None,\n",
    "          initializer=None, initialize_with_model=None, train_mixed_precision=False, efficient_eval_masking=True, **model_extra_args\n",
    "          ):\n",
    "\n",
    "    def eval_pos_seq_len_sampler():\n",
    "        single_eval_pos = single_eval_pos_gen()\n",
    "        if bptt_extra_samples:\n",
    "            return single_eval_pos, single_eval_pos + bptt_extra_samples\n",
    "        else:\n",
    "            return single_eval_pos, bptt\n",
    "\n",
    "    # haven't found this function priordataloader_class() in other docs - where is it defined?        \n",
    "    dl = priordataloader_class(num_steps=steps_per_epoch, \n",
    "                               batch_size=batch_size, \n",
    "                               eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, \n",
    "                               seq_len_maximum=bptt+(bptt_extra_samples if bptt_extra_samples else 0), \n",
    "                               device=device, \n",
    "                               **extra_prior_kwargs_dict)\n",
    "\n",
    "    encoder = encoder_generator(dl.num_features, emsize)\n",
    "    #style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)\n",
    "    style_def = None\n",
    "    #print(f'Style definition of first 3 examples: {style_def[:3] if style_def is not None else None}')\n",
    "    style_encoder = style_encoder_generator(style_def.shape[1], emsize) if (style_def is not None) else None\n",
    "    if isinstance(criterion, nn.GaussianNLLLoss):\n",
    "        n_out = 2\n",
    "    elif isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        n_out = criterion.weight.shape[0]\n",
    "    else:\n",
    "        n_out = 1\n",
    "\n",
    "    model = TransformerModel(encoder, n_out, emsize, nhead, nhid, nlayers, dropout, style_encoder=style_encoder,\n",
    "                             y_encoder=y_encoder_generator(1, emsize), input_normalization=input_normalization,\n",
    "                             pos_encoder=(pos_encoder_generator or positional_encodings.NoPositionalEncoding)(emsize, bptt*2),\n",
    "                             decoder=decoder, init_method=initializer, efficient_eval_masking=efficient_eval_masking, **model_extra_args\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "style_encoder_generator = None\n",
    "\n",
    "style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)\n",
    "style_def = None\n",
    "#print(f'Style definition of first 3 examples: {style_def[:3] if style_def is not None else None}')\n",
    "style_encoder = style_encoder_generator(style_def.shape[1], emsize) if (style_def is not None) else None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) they import encoders so the content of encoders.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "import tabpfn.encoders as encoders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in file encoders.py they have this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "Linear = nn.Linear\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, num_features, emsize, replace_nan_by_zero=False):\n",
    "        super().__init__(num_features, emsize)\n",
    "        self.num_features = num_features\n",
    "        self.emsize = emsize\n",
    "        self.replace_nan_by_zero = replace_nan_by_zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.replace_nan_by_zero:\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "        return super().forward(x)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        self.__dict__.setdefault('replace_nan_by_zero', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "MLP = lambda num_features, emsize: nn.Sequential(nn.Linear(num_features+1,emsize*2),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(emsize*2,emsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "class _PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        self.device_test_tensor = nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "    def forward(self, x):# T x B x num_features\n",
    "        assert self.d_model % x.shape[-1]*2 == 0\n",
    "        d_per_feature = self.d_model // x.shape[-1]\n",
    "        pe = torch.zeros(*x.shape, d_per_feature, device=self.device_test_tensor.device)\n",
    "        #position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        interval_size = 10\n",
    "        div_term = (1./interval_size) * 2*math.pi*torch.exp(torch.arange(0, d_per_feature, 2, device=self.device_test_tensor.device).float()*math.log(math.sqrt(2)))\n",
    "        #print(div_term/2/math.pi)\n",
    "        pe[..., 0::2] = torch.sin(x.unsqueeze(-1) * div_term)\n",
    "        pe[..., 1::2] = torch.cos(x.unsqueeze(-1) * div_term)\n",
    "        return self.dropout(pe).view(x.shape[0],x.shape[1],self.d_model)\n",
    "\n",
    "\n",
    "Positional = lambda _, emsize: _PositionalEncoding(d_model=emsize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) with this we're able to set the encoder and y_encoder to specific values (done through command line as I understand?):\n",
    "\n",
    "* --encoder 'linear' / 'mlp' / 'positional'\n",
    "* --y_encoder 'linear' / 'mlp' / 'positional'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def _parse_args(config_parser, parser):\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_parser = argparse.ArgumentParser(description='Only used as a first parser for the config file path.')\n",
    "    config_parser.add_argument('--config')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--encoder', default='linear', type=str, help='Specify depending on the prior.')\n",
    "    parser.add_argument('--y_encoder', default='linear', type=str, help='Specify depending on the prior. You should specify this if you do not fuse x and y.')\n",
    "    parser.add_argument('--pos_encoder', default='none', type=str, help='Specify depending on the prior.')\n",
    "\n",
    "    args, _ = _parse_args(config_parser, parser)\n",
    "\n",
    "    if args.nhid is None:\n",
    "        args.nhid = 2*args.emsize\n",
    "\n",
    "    encoder = args.__dict__.pop('encoder') # sets encoder to the value of dictionary element 'encoder'\n",
    "    y_encoder = args.__dict__.pop('y_encoder') # sets y_encoder to the value of dictionary element 'y_encoder'\n",
    "\n",
    "    \n",
    "    def get_encoder_generator(encoder):\n",
    "        if encoder == 'linear':\n",
    "            encoder_generator = encoders.Linear\n",
    "        elif encoder == 'mlp':\n",
    "            encoder_generator = encoders.MLP\n",
    "        elif encoder == 'positional':\n",
    "            encoder_generator = encoders.Positional\n",
    "        else:\n",
    "            raise NotImplementedError(f'A {encoder} encoder is not valid.')\n",
    "        return encoder_generator\n",
    "\n",
    "    encoder_generator = get_encoder_generator(encoder)\n",
    "    y_encoder_generator = get_encoder_generator(y_encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding encoders.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class Linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* encodes (linearly) all datapoint vectors into new vectors of length = emsize\n",
    "* replaces NaN by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "\n",
    "    def __init__(self, num_features, emsize, replace_nan_by_zero=False):\n",
    "        super().__init__(num_features, emsize)\n",
    "        self.num_features = num_features\n",
    "        self.emsize = emsize\n",
    "        self.replace_nan_by_zero = replace_nan_by_zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.replace_nan_by_zero:\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (input) shape: torch.Size([1000, 20])\n",
      "output shape: torch.Size([1000, 512])\n",
      "tensor([[ 0.0499,  0.3734, -0.9258,  ...,  0.1520,  0.6031,  0.5477],\n",
      "        [-0.4581, -0.3672,  0.7149,  ...,  0.6977, -0.3434, -0.8434],\n",
      "        [ 0.1711, -0.6576, -0.3224,  ..., -0.9545, -0.2267,  0.0727],\n",
      "        ...,\n",
      "        [-0.8495,  0.9168, -0.2270,  ...,  0.1044,  0.6164,  1.6906],\n",
      "        [-0.0899, -0.5649,  1.0796,  ...,  0.4228, -0.5281, -0.0314],\n",
      "        [ 0.5822,  0.6002, -0.1890,  ..., -0.3911, -0.4425,  0.2561]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_features = 20 # no. of features - no default, depends on the prior, see train.py\n",
    "em_size = 512 # default, see train.py\n",
    "batch = 1000 # default, see train.py\n",
    "x = torch.randn(batch, num_features) # 128 vecs, each of length 20 (so each datapoint has 20 features)\n",
    "\n",
    "encoder_ln = Linear(num_features, em_size)\n",
    "output_ln = encoder_ln.forward(x)\n",
    "\n",
    "print(\"x (input) shape:\", x.shape)\n",
    "print(\"output shape:\", output_ln.size())\n",
    "print(output_ln)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class StyleEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodes numerical features (all features at once)\n",
    "\n",
    "* takes a batch of datapoints and looks at all features - so looks at datapoint vectors of length = num_hyperparameters\n",
    "* encodes (linearly) all datapoint vectors into new vectors of length = em_size\n",
    "\n",
    "essentially does the same as class Linear which additionally encodes NaN to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, num_hyperparameters, em_size):\n",
    "        super().__init__()\n",
    "        self.em_size = em_size\n",
    "        self.embedding = nn.Linear(num_hyperparameters, self.em_size)\n",
    "\n",
    "    def forward(self, hyperparameters):  \n",
    "        return self.embedding(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (input) shape: torch.Size([1000, 20])\n",
      "output shape: torch.Size([1000, 512])\n",
      "tensor([[ 0.0499,  0.3734, -0.9258,  ...,  0.1520,  0.6031,  0.5477],\n",
      "        [-0.4581, -0.3672,  0.7149,  ...,  0.6977, -0.3434, -0.8434],\n",
      "        [ 0.1711, -0.6576, -0.3224,  ..., -0.9545, -0.2267,  0.0727],\n",
      "        ...,\n",
      "        [-0.8495,  0.9168, -0.2270,  ...,  0.1044,  0.6164,  1.6906],\n",
      "        [-0.0899, -0.5649,  1.0796,  ...,  0.4228, -0.5281, -0.0314],\n",
      "        [ 0.5822,  0.6002, -0.1890,  ..., -0.3911, -0.4425,  0.2561]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_hyperparameters = 20 # no. of features - no default, depends on the prior, see train.py\n",
    "em_size = 512 # default, see train.py\n",
    "batch = 1000 # default, see train.py\n",
    "hyperparameters = torch.randn(batch, num_hyperparameters) \n",
    "\n",
    "encoder_st = StyleEncoder(num_hyperparameters, em_size)\n",
    "output_st = encoder_st.forward(hyperparameters)\n",
    "\n",
    "print(\"x (input) shape:\", hyperparameters.shape)\n",
    "print(\"output shape:\", output_st.size())\n",
    "print(output_st)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class StyleEmbEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodes categorical features (one feature at a time)\n",
    "\n",
    "* takes a batch of datapoints and looks at one feature (assert num_hyperparameters == 1)\n",
    "* encodes all possible values of this feature into vectors of length = em_size\n",
    "* note: max number of distinct values that one feature can get is set to num_embeddings=100 (as I understand)\n",
    "\n",
    "pvz: feature f_1: clothing_size={S,M,L}\n",
    "\n",
    "so clothing_size has 3 classes and each class will get its own unique vector of length = em_size\n",
    "\n",
    "we'll have 3 distinct vecs: one for S, one for M and one for L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEmbEncoder(nn.Module):\n",
    "    def __init__(self, num_hyperparameters, em_size, num_embeddings=100):\n",
    "        super().__init__()\n",
    "        assert num_hyperparameters == 1\n",
    "        self.em_size = em_size\n",
    "        self.embedding = nn.Embedding(num_embeddings, self.em_size)\n",
    "\n",
    "    def forward(self, hyperparameters): \n",
    "        return self.embedding(hyperparameters.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [0],\n",
       "        [5],\n",
       "        [9],\n",
       "        [2],\n",
       "        [3],\n",
       "        [0],\n",
       "        [3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 10, (8, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (input) shape: torch.Size([1000, 1])\n",
      "output shape: torch.Size([1000, 512])\n",
      "tensor([[-1.6819,  2.1511, -0.3536,  ..., -0.5102,  1.3916, -0.1969],\n",
      "        [ 0.4755, -0.1734,  0.2474,  ..., -0.7203,  0.9995, -1.2702],\n",
      "        [ 0.2038,  0.9803, -1.2191,  ..., -0.7268,  1.9840, -1.0883],\n",
      "        ...,\n",
      "        [-2.3043,  0.1429, -1.0265,  ...,  0.7998,  0.8934, -0.7810],\n",
      "        [-1.2654,  0.0638,  0.6347,  ...,  0.0913, -0.4807,  0.1304],\n",
      "        [ 0.0595, -1.0513, -1.8355,  ..., -0.0785,  1.0079, -0.2281]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_embeddings = 100 # given in the initialization of the class\n",
    "num_hyperparameters_1 = 1 # no. of features - no default, depends on the prior, see train.py\n",
    "em_size = 512 # default, see train.py\n",
    "batch = 1000 # default, see train.py\n",
    "hyperparameters = torch.randint(0, num_embeddings, (batch, num_hyperparameters_1))\n",
    "\n",
    "encoder_stemb = StyleEmbEncoder(num_hyperparameters_1, em_size)\n",
    "output_stemb = encoder_stemb.forward(hyperparameters)\n",
    "\n",
    "print(\"x (input) shape:\", hyperparameters.shape)\n",
    "print(\"output shape:\", output_stemb.size())\n",
    "print(output_stemb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, so it works like this:\n",
    "\n",
    "StyleEmbEncoder():\n",
    "\n",
    "* num_hyperparameters = 1 - so we take one feature (say datapoint has 5 features, so we take first feature, which is e.g. gender)\n",
    "* this hyperparameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding transformer.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a class `TransformerModel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, n_out, ninp, nhead, nhid, nlayers, dropout=0.0, style_encoder=None, y_encoder=None,\n",
    "                 pos_encoder=None, decoder=None, input_normalization=False, init_method=None, pre_norm=False,\n",
    "                 activation='gelu', recompute_attn=False, num_global_att_tokens=0, full_attention=False,\n",
    "                 all_layers_same_init=False, efficient_eval_masking=True):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        encoder_layer_creator = lambda: TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation=activation,\n",
    "                                                                pre_norm=pre_norm, recompute_attn=recompute_attn)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer_creator(), nlayers)\\\n",
    "            if all_layers_same_init else TransformerEncoderDiffInit(encoder_layer_creator, nlayers)\n",
    "        self.encoder = encoder\n",
    "        self.y_encoder = y_encoder\n",
    "        self.pos_encoder = pos_encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "within this class there's a function `forward()` which uses the argument `encoder` which is an argument for this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "def forward(self, src, src_mask=None, single_eval_pos=None):\n",
    "        assert isinstance(src, tuple), 'inputs (src) have to be given as (x,y) or (style,x,y) tuple'\n",
    "\n",
    "        if len(src) == 2: # (x,y) and no style\n",
    "            src = (None,) + src\n",
    "\n",
    "        style_src, x_src, y_src = src\n",
    "        x_src = self.encoder(x_src)\n",
    "        y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "        style_src = self.style_encoder(style_src).unsqueeze(0) if self.style_encoder else \\\n",
    "            torch.tensor([], device=x_src.device)\n",
    "        global_src = torch.tensor([], device=x_src.device) if self.global_att_embeddings is None else \\\n",
    "            self.global_att_embeddings.weight.unsqueeze(1).repeat(1, x_src.shape[1], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params to TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passed into train() in train.py\n",
    "emsize=512 #yes, same in the paper\n",
    "nhead=4 #yes, same in the paper\n",
    "nhid=2*emsize # #yes, same in the paper: 1024\n",
    "nlayers=6 # hmm, paper says 12\n",
    "\n",
    "# encoder = \n",
    "n_out = 1 # can be 2 or sth else\n",
    "ninp = emsize\n",
    "nhead = nhead\n",
    "nhid = 2*emsize\n",
    "nlayers = nlayers\n",
    "dropout=0.0\n",
    "style_encoder=None\n",
    "y_encoder=None\n",
    "pos_encoder=None\n",
    "decoder=None\n",
    "input_normalization=False\n",
    "init_method=None\n",
    "pre_norm=False\n",
    "activation='gelu'\n",
    "recompute_attn=False\n",
    "num_global_att_tokens=0\n",
    "full_attention=False\n",
    "all_layers_same_init=False\n",
    "efficient_eval_masking=True\n",
    "\n",
    "num_features = 10 # no default, depends on the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerEncoderDiffInit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "from torch.nn import Module, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDiffInit(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "\n",
    "    Args:\n",
    "        encoder_layer_creator: a function generating objects of TransformerEncoderLayer class without args (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer_creator, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer_creator() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_global_att_query_matrix()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this dunction is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_mask_to_att_mask(mask):\n",
    "    return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_att_query_matrix(num_global_att_tokens, seq_len, num_query_tokens):\n",
    "    \n",
    "    \"\"\"Generates matrix with row for each query explaining which points it should attend\n",
    "    includes itself.\n",
    "\n",
    "    Args:\n",
    "        num_global_att_tokens (int): \n",
    "        seq_len (int): number of points in batch (I believe)\n",
    "        num_query_tokens (int): \n",
    "\n",
    "    Returns:\n",
    "        mask: num_query_tokens x (seq_len + num_global_att_tokens - num_query_tokens) \n",
    "\n",
    "    \"\"\"\n",
    "    train_size = seq_len + num_global_att_tokens - num_query_tokens\n",
    "    sz = seq_len + num_global_att_tokens\n",
    "    mask = torch.zeros(num_query_tokens, sz) == 0\n",
    "    mask[:,train_size:].zero_()\n",
    "    mask[:,train_size:] |= torch.eye(num_query_tokens) == 1\n",
    "    return bool_mask_to_att_mask(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is used in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "src_mask_args = (global_att_embeddings.num_embeddings,\n",
    "                                 len(x_src) + len(style_src),\n",
    "                                 len(x_src) + len(style_src) - single_eval_pos)\n",
    "src_mask = (generate_global_att_globaltokens_matrix(*src_mask_args).to(x_src.device),\n",
    "            sgenerate_global_att_trainset_matrix(*src_mask_args).to(x_src.device),\n",
    "            generate_global_att_query_matrix(*src_mask_args).to(x_src.device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting src_mask_args which we need as arguments in generate_global_att_query_matrix(*src_mask_args).to(x_src.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_global_att_tokens are NOT None\n"
     ]
    }
   ],
   "source": [
    "if num_global_att_tokens:\n",
    "    print(\"num_global_att_tokens are None\")\n",
    "else:\n",
    "    print(\"num_global_att_tokens are NOT None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "global_att_embeddings = nn.Embedding(num_global_att_tokens, ninp) if num_global_att_tokens else None\n",
    "print(global_att_embeddings==None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "src_mask_args = (global_att_embeddings.num_embeddings,\n",
    "                                 len(x_src) + len(style_src),\n",
    "                                 len(x_src) + len(style_src) - single_eval_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_D_q_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 10])\n",
      "torch.Size([3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(3, 4, 10).shape)\n",
    "print(torch.rand(3, 4, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = (torch.rand(3, 4, 10), torch.rand(3, 4, 1)) # that's a tuple (x,y)\n",
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for encoder and y_encoder\n",
    "Linear = nn.Linear\n",
    "MLP = lambda num_features, emsize: nn.Sequential(nn.Linear(num_features+1,emsize*2), nn.ReLU(), nn.Linear(emsize*2,emsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEmbEncoder(nn.Module):\n",
    "    r\"\"\" Uses the nn.Embedding function to emmbed the given \"hyperparameter\"\n",
    "        So it encodes categorical features (one feature at a time):\n",
    "        * takes a batch of datapoints and looks at one feature (assert num_hyperparameters == 1)\n",
    "        * encodes all possible values of this feature into vectors of length = em_size\n",
    "        * note: max number of distinct values that one feature can get is set to num_embeddings=100 (as I understand)\n",
    "        \"\"\"\n",
    "    def __init__(self, num_hyperparameters, em_size, num_embeddings=100):\n",
    "                \n",
    "        super().__init__()\n",
    "        assert num_hyperparameters == 1\n",
    "        self.em_size = em_size\n",
    "        self.embedding = nn.Embedding(num_embeddings, self.em_size)\n",
    "\n",
    "    def forward(self, hyperparameters):  # Batch x num_hyperparameters\n",
    "        \n",
    "        return self.embedding(hyperparameters.squeeze(1))\n",
    "\n",
    "StyleEmbEncoder = StyleEmbEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for style_encoder\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "#from .prior import PriorDataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import gpytorch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PriorDataLoader(DataLoader):\n",
    "    pass\n",
    "    # init accepts num_steps as first argument\n",
    "\n",
    "    # has two attributes set on class or object level:\n",
    "    # num_features: int and\n",
    "    # num_outputs: int\n",
    "    # fuse_x_y: bool\n",
    "    # Optional: validate function that accepts a transformer model\n",
    "\n",
    "def get_batch_to_dataloader(get_batch_method_):\n",
    "    class DL(PriorDataLoader):\n",
    "        get_batch_method = get_batch_method_\n",
    "\n",
    "        num_features = 20 # I've set this manually\n",
    "\n",
    "        # Caution, you might need to set self.num_features manually if it is not part of the args.\n",
    "        def __init__(self, num_steps, **get_batch_kwargs):\n",
    "            set_locals_in_self(locals())\n",
    "\n",
    "            # The stuff outside the or is set as class attribute before instantiation.\n",
    "            self.num_features = get_batch_kwargs.get('num_features') or self.num_features\n",
    "            self.epoch_count = 0\n",
    "            #print('DataLoader.__dict__', self.__dict__)\n",
    "\n",
    "        @staticmethod\n",
    "        def gbm(*args, eval_pos_seq_len_sampler, **kwargs):\n",
    "            kwargs['single_eval_pos'], kwargs['seq_len'] = eval_pos_seq_len_sampler()\n",
    "            # Scales the batch size dynamically with the power of 'dynamic_batch_size'.\n",
    "            # A transformer with quadratic memory usage in the seq len would need a power of 2 to keep memory constant.\n",
    "            if 'dynamic_batch_size' in kwargs and kwargs['dynamic_batch_size'] > 0 and kwargs['dynamic_batch_size']:\n",
    "                kwargs['batch_size'] = kwargs['batch_size'] * math.floor(math.pow(kwargs['seq_len_maximum'], kwargs['dynamic_batch_size']) / math.pow(kwargs['seq_len'], kwargs['dynamic_batch_size']))\n",
    "            batch = get_batch_method_(*args, **kwargs)\n",
    "            x, y, target_y, style = batch if len(batch) == 4 else (batch[0], batch[1], batch[2], None)\n",
    "            return (style, x, y), target_y, kwargs['single_eval_pos']\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_steps\n",
    "\n",
    "        def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "        def __iter__(self):\n",
    "            assert hasattr(self, 'model'), \"Please assign model with `dl.model = ...` before training.\"\n",
    "            self.epoch_count += 1\n",
    "            return iter(self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count - 1, model=self.model) for _ in range(self.num_steps))\n",
    "\n",
    "    return DL\n",
    "\n",
    "def set_locals_in_self(locals):\n",
    "    \"\"\"\n",
    "    Call this function like `set_locals_in_self(locals())` to set all local variables as object variables.\n",
    "    Especially useful right at the beginning of `__init__`.\n",
    "    :param locals: `locals()`\n",
    "    \"\"\"\n",
    "    self = locals['self']\n",
    "    for var_name, val in locals.items():\n",
    "        if var_name != 'self': setattr(self, var_name, val)\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def get_model(x, y, hyperparameters):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.GreaterThan(1.e-9))\n",
    "    model = ExactGPModel(x, y, likelihood)\n",
    "    model.likelihood.noise = torch.ones_like(model.likelihood.noise) * hyperparameters[\"noise\"]\n",
    "    model.covar_module.outputscale = torch.ones_like(model.covar_module.outputscale) * hyperparameters[\"outputscale\"]\n",
    "    model.covar_module.base_kernel.lengthscale = torch.ones_like(model.covar_module.base_kernel.lengthscale) * \\\n",
    "                                                 hyperparameters[\"lengthscale\"]\n",
    "    return model, likelihood\n",
    "\n",
    "\n",
    "default_device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batch(batch_size, seq_len, num_features=20, device=default_device, hyperparameters=None, # num_features=10 had to add a number manually (chose 20 just randomly)\n",
    "              equidistant_x=False, fix_x=None, **kwargs):\n",
    "    if isinstance(hyperparameters, (tuple, list)):\n",
    "        hyperparameters = {\"noise\": hyperparameters[0]\n",
    "            , \"outputscale\": hyperparameters[1]\n",
    "            , \"lengthscale\": hyperparameters[2]\n",
    "            , \"is_binary_classification\": hyperparameters[3]\n",
    "            # , \"num_features_used\": hyperparameters[4]\n",
    "            , \"normalize_by_used_features\": hyperparameters[5]\n",
    "            , \"order_y\": hyperparameters[6]\n",
    "            , \"sampling\": hyperparameters[7]\n",
    "                           }\n",
    "    elif hyperparameters is None:\n",
    "        hyperparameters = {\"noise\": .1, \"outputscale\": .1, \"lengthscale\": .1}\n",
    "\n",
    "    if 'verbose' in hyperparameters and hyperparameters['verbose']:\n",
    "        print({\"noise\": hyperparameters['noise'], \"outputscale\": hyperparameters['outputscale']\n",
    "                  , \"lengthscale\": hyperparameters['lengthscale'], 'batch_size': batch_size, 'sampling': hyperparameters['sampling']})\n",
    "\n",
    "    # hyperparameters = {k: hyperparameters[k]() if callable(hyperparameters[k]) else hyperparameters[k] for k in\n",
    "    #      hyperparameters.keys()}\n",
    "    assert not (equidistant_x and (fix_x is not None))\n",
    "\n",
    "    with gpytorch.settings.fast_computations(*hyperparameters.get('fast_computations', (True, True, True))):\n",
    "        if equidistant_x:\n",
    "            assert num_features == 1\n",
    "            x = torch.linspace(0, 1., seq_len).unsqueeze(0).repeat(batch_size, 1).unsqueeze(-1)\n",
    "        elif fix_x is not None:\n",
    "            assert fix_x.shape == (seq_len, num_features)\n",
    "            x = fix_x.unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "        else:\n",
    "            if hyperparameters.get('sampling','uniform') == 'uniform':\n",
    "                x = torch.rand(batch_size, seq_len, num_features, device=device)\n",
    "            else:\n",
    "                x = torch.randn(batch_size, seq_len, num_features, device=device)\n",
    "        model, likelihood = get_model(x, torch.Tensor(), hyperparameters)\n",
    "        model.to(device)\n",
    "        # trained_model = ExactGPModel(train_x, train_y, likelihood).cuda()\n",
    "        # trained_model.eval()\n",
    "        is_fitted = False\n",
    "        while not is_fitted:\n",
    "            try:\n",
    "                with gpytorch.settings.prior_mode(True):\n",
    "                    model, likelihood = get_model(x, torch.Tensor(), hyperparameters)\n",
    "                    model.to(device)\n",
    "\n",
    "                    d = model(x)\n",
    "                    d = likelihood(d)\n",
    "                    sample = d.sample().transpose(0, 1)\n",
    "                    is_fitted = True\n",
    "            except RuntimeError: # This can happen when torch.linalg.eigh fails. Restart with new init resolves this.\n",
    "                print('GP Fitting unsuccessful, retrying.. ')\n",
    "                print(x)\n",
    "                print(hyperparameters)\n",
    "\n",
    "    if bool(torch.any(torch.isnan(x)).detach().cpu().numpy()):\n",
    "        print({\"noise\": hyperparameters['noise'], \"outputscale\": hyperparameters['outputscale']\n",
    "                  , \"lengthscale\": hyperparameters['lengthscale'], 'batch_size': batch_size})\n",
    "\n",
    "    # TODO: Multi output\n",
    "    return x.transpose(0, 1), sample, sample  # x.shape = (T,B,H)\n",
    "\n",
    "DataLoader = get_batch_to_dataloader(get_batch)\n",
    "\n",
    "priordataloader_class = DataLoader\n",
    "\n",
    "steps_per_epoch = 100 # set to 10\n",
    "batch_size = 200 # set to 1000\n",
    "bptt=10 # default in function train(), not changed afterwards\n",
    "bptt_extra_samples=None # default in function train(), not changed afterwards\n",
    "single_eval_pos_gen=None # default in function train(), not changed afterwards\n",
    "extra_prior_kwargs_dict={} # default in function train(), not changed afterwards\n",
    "gpu_device='cuda:0' # default in function train(), not changed afterwards\n",
    "device = gpu_device if torch.cuda.is_available() else 'cpu:0'\n",
    "single_eval_pos_gen=None\n",
    "\n",
    "def eval_pos_seq_len_sampler():\n",
    "    single_eval_pos = single_eval_pos_gen()\n",
    "    if bptt_extra_samples:\n",
    "        return single_eval_pos, single_eval_pos + bptt_extra_samples\n",
    "    else:\n",
    "        return single_eval_pos, bptt\n",
    "\n",
    "def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "\n",
    "dl = priordataloader_class(num_steps=steps_per_epoch, batch_size=batch_size, eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, seq_len_maximum=bptt+(bptt_extra_samples if bptt_extra_samples else 0), device=device, **extra_prior_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_single_eval_pos_sampler(max_len, min_len=0):\n",
    "    \"\"\"\n",
    "    Just sample any evaluation position with the same weight\n",
    "    :return: Sampler that can be fed to `train()` as `single_eval_pos_gen`.\n",
    "    \"\"\"\n",
    "    return lambda: random.choices(range(min_len, max_len))[0]\n",
    "\n",
    "get_sampler = get_uniform_single_eval_pos_sampler\n",
    "permutation_invariant_max_eval_pos = 100 # very random, had to set it to sth but don't know what this is\n",
    "\n",
    "single_eval_pos_gen = get_sampler(permutation_invariant_max_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_generator = Linear\n",
    "y_encoder_generator = Linear\n",
    "style_encoder_generator = StyleEmbEncoder # not sure if this makes sense :)\n",
    "\n",
    "\n",
    "encoder = encoder_generator(num_features, emsize)\n",
    "y_encoder = y_encoder_generator(1, emsize)\n",
    "style_encoder = style_encoder_generator(style_def.shape[1], emsize) if (style_def is not None) else None\n",
    "# style_encoder = None by default"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dl.get_test_batch() is ((style, x, y), target, single_eval_pos)\n",
    "\n",
    "dl.get_test_batch()[0] gives (style, x, y)\n",
    "\n",
    "dl.get_test_batch()[0][0] gives style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gp = dl.get_test_batch()\n",
    "#data_gp.to_csv(\"data_gp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "torch.Size([10, 200, 20])\n",
      "torch.Size([10, 200])\n"
     ]
    }
   ],
   "source": [
    "# tuple: (style, x, y)\n",
    "print(dl.get_test_batch()[0][0]) # style: seems like it's None :(\n",
    "print(dl.get_test_batch()[0][1].shape) # x: seems like its a batch of 10 samples where each has 200 x vectors each with 20 features\n",
    "print(dl.get_test_batch()[0][2].shape) # y: seems like its a batch of 10 samples where each has 200 x vectors of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9385, 0.2227, 0.1930, 0.0740, 0.7435],\n",
      "        [0.3993, 0.1896, 0.7265, 0.7708, 0.3506],\n",
      "        [0.1995, 0.7789, 0.8519, 0.1545, 0.7565],\n",
      "        [0.3894, 0.9077, 0.4360, 0.8205, 0.4249]])\n",
      "tensor([ 0.1707, -0.1182, -0.3713, -0.0126])\n"
     ]
    }
   ],
   "source": [
    "print(dl.get_test_batch()[0][1][0,0:4,0:5]) # rows are vecs x1, x2, x3, x4\n",
    "print(dl.get_test_batch()[0][2][0,0:4]) # elements are values y1, y2, y3, y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(src) == 2: # (x,y) and no style\n",
    "            src = (None,) + src\n",
    "\n",
    "style_src, x_src, y_src = src\n",
    "x_src = encoder(x_src)\n",
    "y_src = y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)\n",
    "style_src = style_encoder(style_src).unsqueeze(0) if style_encoder else torch.tensor([], device=x_src.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_len = len(x_src) + len(style_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bool_mask_to_att_mask(mask):\n",
    "    return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_D_q_matrix(sz, query_size):\n",
    "        \"\"\"Generates same attnetion matrix as in paper (first one\n",
    "        with the diagonal being one) except all 1 entries are 0.0 \n",
    "        and 0 entries are -inf\n",
    "\n",
    "        Args:\n",
    "            sz (int): size of the input - x and y\n",
    "            query_size (int): size of x\n",
    "\n",
    "        Returns:\n",
    "            tensor: mask that masks y but attends itself (diagonal 0.0 NOT -inf)\n",
    "        \"\"\"\n",
    "        train_size = sz-query_size\n",
    "        mask = torch.zeros(sz,sz) == 0\n",
    "        mask[:,train_size:].zero_()\n",
    "        mask |= torch.eye(sz) == 1\n",
    "        return bool_mask_to_att_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_D_q_matrix(full_len, len(x_src) - single_eval_pos).to(x_src.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "data = torch.rand(10, 32, 512)\n",
    "query, key, value = data, data, data \n",
    "\n",
    "embed_dim = 512\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class TransformerEncoderLayer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# full\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``.\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state): # not sure what it does\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False by default and is not changed in model=TransformerModel() in train.py\n",
    "            src_ = self.norm1(src)\n",
    "            #print(\"not run\")\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        if isinstance(src_mask, tuple): # NOT RUN - AssertionError \n",
    "            # global attention setup\n",
    "            assert not self.self_attn.batch_first # AssertionError when batch_first=True: not True = False  --> so batch_first must be False (and it is - default False is not changed in model=TransformerModel() in train.py)\n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            \n",
    "            # I think this is not run as we get AssertionError: default src_key_padding_mask=None is not changed\n",
    "            # so we actually do what's in else (elif also gets AssertionError fot the same reason)\n",
    "            \n",
    "            global_src_mask, trainset_src_mask, valset_src_mask = src_mask\n",
    "\n",
    "            num_global_tokens = global_src_mask.shape[0]\n",
    "            num_train_tokens = trainset_src_mask.shape[0]\n",
    "\n",
    "            global_tokens_src = src_[:num_global_tokens]\n",
    "            train_tokens_src = src_[num_global_tokens:num_global_tokens+num_train_tokens]\n",
    "            global_and_train_tokens_src = src_[:num_global_tokens+num_train_tokens]\n",
    "            eval_tokens_src = src_[num_global_tokens+num_train_tokens:]\n",
    "\n",
    "\n",
    "            attn = partial(checkpoint, self.self_attn) if self.recompute_attn else self.self_attn\n",
    "\n",
    "            global_tokens_src2 = attn(global_tokens_src, global_and_train_tokens_src, global_and_train_tokens_src, None, True, global_src_mask)[0]\n",
    "            train_tokens_src2 = attn(train_tokens_src, global_tokens_src, global_tokens_src, None, True, trainset_src_mask)[0]\n",
    "            eval_tokens_src2 = attn(eval_tokens_src, src_, src_,\n",
    "                                    None, True, valset_src_mask)[0]\n",
    "\n",
    "            src2 = torch.cat([global_tokens_src2, train_tokens_src2, eval_tokens_src2], dim=0)\n",
    "\n",
    "        elif isinstance(src_mask, int): # NOT RUN - AssertionError \n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            single_eval_position = src_mask\n",
    "            src_left = self.self_attn(src_[:single_eval_position], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src_right = self.self_attn(src_[single_eval_position:], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            src2 = torch.cat([src_left, src_right], dim=0)\n",
    "        else: # this gets RUN \n",
    "            if self.recompute_attn: # recompute_attn=False by default, and is not changed in model=TransformerModel() in train.py)\n",
    "                src2 = checkpoint(self.self_attn, src_, src_, src_, src_key_padding_mask, True, src_mask)[0]\n",
    "            else: # so we actually do this part\n",
    "                src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False\n",
    "            src_ = self.norm2(src)\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src_))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm2(src)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.2793, 0.4031])\n",
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_full = encoder_layer(src)\n",
    "\n",
    "print(src[0,0,0:3])\n",
    "print(out_full.shape)\n",
    "print(out_full[0,0,0:3])\n",
    "# tensor([ 1.2991, -0.8532, -0.0118]) # when I run with full class definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# commented out what's not run\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False``.\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    # def __setstate__(self, state): # not sure what it does\n",
    "    #     if 'activation' not in state:\n",
    "    #         state['activation'] = F.relu\n",
    "    #     super().__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False by default and is not changed in model=TransformerModel() in train.py\n",
    "            # src_ = self.norm1(src)\n",
    "            print(\"not run\")\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        \n",
    "        if isinstance(src_mask, tuple): # NOT RUN - AssertionError \n",
    "            # global attention setup\n",
    "            assert not self.self_attn.batch_first # AssertionError when batch_first=True: not True = False  --> so batch_first must be False (and it is - default False is not changed in model=TransformerModel() in train.py)\n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            \n",
    "            # I think this is not run as we get AssertionError: default src_key_padding_mask=None is not changed\n",
    "            # so we actually do what's in else (elif also gets AssertionError fot the same reason)\n",
    "            \n",
    "            # global_src_mask, trainset_src_mask, valset_src_mask = src_mask\n",
    "\n",
    "            # num_global_tokens = global_src_mask.shape[0]\n",
    "            # num_train_tokens = trainset_src_mask.shape[0]\n",
    "\n",
    "            # global_tokens_src = src_[:num_global_tokens]\n",
    "            # train_tokens_src = src_[num_global_tokens:num_global_tokens+num_train_tokens]\n",
    "            # global_and_train_tokens_src = src_[:num_global_tokens+num_train_tokens]\n",
    "            # eval_tokens_src = src_[num_global_tokens+num_train_tokens:]\n",
    "\n",
    "\n",
    "            # attn = partial(checkpoint, self.self_attn) if self.recompute_attn else self.self_attn\n",
    "\n",
    "            # global_tokens_src2 = attn(global_tokens_src, global_and_train_tokens_src, global_and_train_tokens_src, None, True, global_src_mask)[0]\n",
    "            # train_tokens_src2 = attn(train_tokens_src, global_tokens_src, global_tokens_src, None, True, trainset_src_mask)[0]\n",
    "            # eval_tokens_src2 = attn(eval_tokens_src, src_, src_,\n",
    "            #                         None, True, valset_src_mask)[0]\n",
    "\n",
    "            # src2 = torch.cat([global_tokens_src2, train_tokens_src2, eval_tokens_src2], dim=0)\n",
    "        elif isinstance(src_mask, int): # NOT RUN - AssertionError \n",
    "            assert src_key_padding_mask is None # AssertionError when src_key_padding_mask=None --> so src_key_padding_mask must be not None (but it is None - default None is not changed)\n",
    "            # single_eval_position = src_mask\n",
    "            # src_left = self.self_attn(src_[:single_eval_position], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            # src_right = self.self_attn(src_[single_eval_position:], src_[:single_eval_position], src_[:single_eval_position])[0]\n",
    "            # src2 = torch.cat([src_left, src_right], dim=0)\n",
    "        else: # this gets RUN \n",
    "            if self.recompute_attn: # recompute_attn=False by default, and is not changed in model=TransformerModel() in train.py)\n",
    "                # src2 = checkpoint(self.self_attn, src_, src_, src_, src_key_padding_mask, True, src_mask)[0]\n",
    "                print(\"not run\")\n",
    "            else: # so we actually do this part\n",
    "                src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                      key_padding_mask=src_key_padding_mask)[0]\n",
    "        \n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm1(src)\n",
    "\n",
    "        if self.pre_norm: # NOT RUN: pre_norm=False\n",
    "            src_ = self.norm2(src)\n",
    "        else: # this gets RUN\n",
    "            src_ = src\n",
    "        \n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src_))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        if not self.pre_norm: # this gets RUN: pre_norm=False so not False is True\n",
    "            src = self.norm2(src)\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out = encoder_layer(src)\n",
    "\n",
    "print(out.shape)\n",
    "print(out[0,0,0:3])\n",
    "# tensor([ 1.2991, -0.8532, -0.0118]) # when I run with full class definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.transformer import _get_activation_fn, Module, Tensor, Optional, MultiheadAttention, Linear, Dropout, LayerNorm\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# added by Ugne (before it showed error: F is not defined)\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# commented out what's not run\n",
    "class DelTransformerEncoderLayer(Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5, batch_first=False, pre_norm=False,\n",
    "                 device=None, dtype=None, recompute_attn=False) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout_ch = Dropout(dropout) # dropout -> dropout_ch\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        self.recompute_attn = recompute_attn\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \n",
    "        # multihead attention\n",
    "        src_ = src\n",
    "        src2 = self.self_attn(src_, src_, src_, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # feed forward\n",
    "        src_ = src\n",
    "        src2 = self.linear2(self.dropout_ch(self.activation(self.linear1(src_)))) # dropout -> dropout_ch\n",
    "        \n",
    "        # add and normalize\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "encoder_layer_del = DelTransformerEncoderLayer(d_model=512, nhead=4)\n",
    "out_deleted = encoder_layer_del(src)\n",
    "\n",
    "print(out_deleted.shape)\n",
    "print(out_deleted[0,0,0:3]) # tensor([ 1.2991, -0.8532, -0.0118]) # when I run with only what I think it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n",
      "tensor([ 1.2991, -0.8532, -0.0118], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "#comparison\n",
    "\n",
    "print(out_full.shape)\n",
    "print(out_deleted.shape)\n",
    "print(out_full[0,0,0:3]) # tensor([ 1.2991, -0.8532, -0.0118]) # when I run with full class definition\n",
    "print(out_deleted[0,0,0:3]) # tensor([ 1.2991, -0.8532, -0.0118]) # when I run with only what I think it does"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.2793, 0.4031])\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512) # 10 batches where each has 32 datapoints so vecs of length = 512\n",
    "\n",
    "print(src[0,0,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "# passed into train() in train.py\n",
    "emsize=512 #yes, same in the paper\n",
    "nhead=4 #yes, same in the paper\n",
    "nhid=2*emsize # #yes, same in the paper: 1024\n",
    "\n",
    "# # def train() in train.py\n",
    "# emsize=200 # function default \n",
    "# nhead=2 # function default\n",
    "# nhid=200 # function default\n",
    "\n",
    "# model = TransformerModel() in train.py\n",
    "emsize = emsize\n",
    "nhead = nhead\n",
    "nhid = nhid\n",
    "\n",
    "# class TransformerModel() in transformer.py\n",
    "ninp = emsize # ninp - number of inputs\n",
    "nhead = nhead\n",
    "nhid = nhid\n",
    "\n",
    "# class TransformerEncoderLayer() in layer.py\n",
    "# d_model = ninp\n",
    "# nhead = nhead\n",
    "# dim_feedforward = nhid\n",
    "d_model = 512\n",
    "nhead = 4\n",
    "dim_feedforward=2048\n",
    "dropout=0.1\n",
    "activation=\"relu\"\n",
    "layer_norm_eps=1e-5\n",
    "batch_first=False\n",
    "pre_norm=False\n",
    "device=None\n",
    "dtype=None\n",
    "recompute_attn=False\n",
    "factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "# def forward() in class TransformerEncoderLayer() in layer.py\n",
    "src_mask = None\n",
    "src_key_padding_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Feedforward model\n",
    "linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "dropout_changed = nn.Dropout(dropout) # changed variable name because of TypeError: '<' not supported between instances of 'Dropout' and 'int'\n",
    "linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "norm1 =nn. LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "dropout1 = nn.Dropout(dropout)\n",
    "dropout2 = nn.Dropout(dropout) \n",
    "pre_norm = pre_norm\n",
    "recompute_attn = recompute_attn\n",
    "\n",
    "activation = _get_activation_fn(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs)\n",
    "torch.manual_seed(1)\n",
    "src_ = src\n",
    "        \n",
    "src2 = self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                key_padding_mask=src_key_padding_mask)[0]\n",
    "\n",
    "src = src + dropout1(src2)\n",
    "\n",
    "\n",
    "src = norm1(src)\n",
    "\n",
    "\n",
    "src_ = src\n",
    "\n",
    "src2 = linear2(dropout_changed(activation(linear1(src_)))) # changed variable name because of TypeError: '<' not supported between instances of 'Dropout' and 'int'\n",
    "src = src + dropout2(src2)\n",
    "\n",
    "src = norm2(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 0.8133, -0.5130,  0.4350], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(src.shape)\n",
    "print(src[0,0,0:3])\n",
    "# tensor([-0.4884, -0.7499,  0.4478])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "nhead = 4\n",
    "dim_feedforward=2048\n",
    "dropout=0.1\n",
    "activation=\"relu\"\n",
    "layer_norm_eps=1e-5\n",
    "batch_first=False\n",
    "pre_norm=False\n",
    "device=None\n",
    "dtype=None\n",
    "recompute_attn=False\n",
    "        \n",
    "factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "\n",
    "self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first, **factory_kwargs)\n",
    "\n",
    "# Implementation of Feedforward model\n",
    "linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "dropout_cha = Dropout(dropout)\n",
    "linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "dropout1 = Dropout(dropout)\n",
    "dropout2 = Dropout(dropout)\n",
    "pre_norm = pre_norm\n",
    "recompute_attn = recompute_attn\n",
    "\n",
    "activation = _get_activation_fn(activation)\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "src = torch.rand(10, 32, 512)\n",
    "\n",
    "src_mask = None\n",
    "src_key_padding_mask = None\n",
    "\n",
    "src_ = src\n",
    "\n",
    "src2 = self_attn(src_, src_, src_, attn_mask=src_mask,\n",
    "                                key_padding_mask=src_key_padding_mask)[0]\n",
    "\n",
    "src = src + dropout1(src2)\n",
    "\n",
    "src = norm1(src)\n",
    "\n",
    "src_ = src\n",
    "\n",
    "src2 = linear2(dropout_cha(activation(linear1(src_))))\n",
    "src = src + dropout2(src2)\n",
    "\n",
    "src = norm2(src)\n",
    "\n",
    "src_out_trial3 = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n",
      "tensor([ 1.3199, -0.2352,  0.3443], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(src_out_trial3.shape)\n",
    "print(src_out_trial3[0,0,0:3]) # tensor([ 0.7857, -1.1753,  0.0188])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globally"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a function `train()` in train.py\n",
    "\n",
    "One of its arguments is encoder_generator - a class (Linear/MLP/Positional from encoders.py)\n",
    "\n",
    "Then in this function train() we create an object of class encoder_generator: `encoder` = encoder_generator(dl.num_features, emsize)\n",
    "\n",
    "pvz.: \n",
    "\n",
    "* encoder_generator = encoders.Linear\n",
    "* encoder = encoders.Linear(num_features=[default 512], emzise=[depends on the prior])\n",
    "\n",
    "With this object encoder we can encode a given dataset x, pvz give 100 datapoints with 20 features in each of them (then emsize=20) and if we do x_encoded=encoder.forward(x) then we get an encoded dataset x_encoded with 100 datapoints where each datapoint now has 512 elements in it\n",
    "\n",
    "Then this function train() sets model = TransformerModel() - an object of class `TransformerModel()` from transformer.py\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a class `TransformerModel()` in transformer.py\n",
    "\n",
    "Within this class there's a function `forward()` which uses arguments `encoder` and `y_encoder` which are the arguments for this class\n",
    "\n",
    "Then funciton forward() basically performs encoding of source datapoints (x,y):\n",
    "\n",
    "* x_src = self.encoder(x_src)\n",
    "* y_src = self.y_encoder(y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3 | packaged by conda-forge | (default, Mar 27 2019, 15:43:19) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
