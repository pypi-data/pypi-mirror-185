{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors/mlp.py\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20 # setting this manually\n",
    "\n",
    "class PriorDataLoader(DataLoader):\n",
    "    pass\n",
    "    # init accepts num_steps as first argument\n",
    "\n",
    "    # has two attributes set on class or object level:\n",
    "    # num_features: int and\n",
    "    # num_outputs: int\n",
    "    # fuse_x_y: bool\n",
    "    # Optional: validate function that accepts a transformer model\n",
    "\n",
    "# tabpfn/utils.py\n",
    "def set_locals_in_self(locals):\n",
    "    \"\"\"\n",
    "    Call this function like `set_locals_in_self(locals())` to set all local variables as object variables.\n",
    "    Especially useful right at the beginning of `__init__`.\n",
    "    :param locals: `locals()`\n",
    "    \"\"\"\n",
    "    self = locals['self']\n",
    "    for var_name, val in locals.items():\n",
    "        if var_name != 'self': setattr(self, var_name, val)\n",
    "\n",
    "default_device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "\n",
    "# priors/utils.py\n",
    "def get_batch_to_dataloader(get_batch_method_):\n",
    "    class DL(PriorDataLoader):\n",
    "        get_batch_method = get_batch_method_\n",
    "\n",
    "        num_features = num_features\n",
    "\n",
    "        # Caution, you might need to set self.num_features manually if it is not part of the args.\n",
    "        def __init__(self, num_steps, **get_batch_kwargs):\n",
    "            set_locals_in_self(locals())\n",
    "\n",
    "            # The stuff outside the or is set as class attribute before instantiation.\n",
    "            self.num_features = get_batch_kwargs.get('num_features') or self.num_features\n",
    "            self.epoch_count = 0\n",
    "            #print('DataLoader.__dict__', self.__dict__)\n",
    "\n",
    "        @staticmethod\n",
    "        def gbm(*args, eval_pos_seq_len_sampler, **kwargs):\n",
    "            kwargs['single_eval_pos'], kwargs['seq_len'] = eval_pos_seq_len_sampler()\n",
    "            # Scales the batch size dynamically with the power of 'dynamic_batch_size'.\n",
    "            # A transformer with quadratic memory usage in the seq len would need a power of 2 to keep memory constant.\n",
    "            if 'dynamic_batch_size' in kwargs and kwargs['dynamic_batch_size'] > 0 and kwargs['dynamic_batch_size']:\n",
    "                kwargs['batch_size'] = kwargs['batch_size'] * math.floor(math.pow(kwargs['seq_len_maximum'], kwargs['dynamic_batch_size']) / math.pow(kwargs['seq_len'], kwargs['dynamic_batch_size']))\n",
    "            batch = get_batch_method_(*args, **kwargs)\n",
    "            x, y, target_y, style = batch if len(batch) == 4 else (batch[0], batch[1], batch[2], None)\n",
    "            return (style, x, y), target_y, kwargs['single_eval_pos']\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_steps\n",
    "\n",
    "        def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "        def __iter__(self):\n",
    "            assert hasattr(self, 'model'), \"Please assign model with `dl.model = ...` before training.\"\n",
    "            self.epoch_count += 1\n",
    "            return iter(self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count - 1, model=self.model) for _ in range(self.num_steps))\n",
    "\n",
    "    return DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors/mlp.py\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std, device):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + torch.normal(torch.zeros_like(x), self.std)\n",
    "\n",
    "\n",
    "def causes_sampler_f(num_causes):\n",
    "    means = np.random.normal(0, 1, (num_causes))\n",
    "    std = np.abs(np.random.normal(0, 1, (num_causes)) * means)\n",
    "    return means, std\n",
    "\n",
    "# manually setting num_features=20\n",
    "def get_batch(batch_size, seq_len, num_features, hyperparameters, device=default_device, num_outputs=1, sampling='normal'\n",
    "              , epoch=None, **kwargs):\n",
    "    if 'multiclass_type' in hyperparameters and hyperparameters['multiclass_type'] == 'multi_node':\n",
    "        num_outputs = num_outputs * hyperparameters['num_classes']\n",
    "\n",
    "    if not (('mix_activations' in hyperparameters) and hyperparameters['mix_activations']):\n",
    "        s = hyperparameters['prior_mlp_activations']()\n",
    "        hyperparameters['prior_mlp_activations'] = lambda : s\n",
    "\n",
    "    class MLP(torch.nn.Module):\n",
    "        def __init__(self, hyperparameters):\n",
    "            super(MLP, self).__init__()\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for key in hyperparameters:\n",
    "                    setattr(self, key, hyperparameters[key])\n",
    "\n",
    "                assert (self.num_layers >= 2)\n",
    "\n",
    "                if 'verbose' in hyperparameters and self.verbose:\n",
    "                    print({k : hyperparameters[k] for k in ['is_causal', 'num_causes', 'prior_mlp_hidden_dim'\n",
    "                        , 'num_layers', 'noise_std', 'y_is_effect', 'pre_sample_weights', 'prior_mlp_dropout_prob'\n",
    "                        , 'pre_sample_causes']})\n",
    "\n",
    "                if self.is_causal:\n",
    "                    self.prior_mlp_hidden_dim = max(self.prior_mlp_hidden_dim, num_outputs + 2 * num_features)\n",
    "                else:\n",
    "                    self.num_causes = num_features\n",
    "\n",
    "                # This means that the mean and standard deviation of each cause is determined in advance\n",
    "                if self.pre_sample_causes:\n",
    "                    self.causes_mean, self.causes_std = causes_sampler_f(self.num_causes)\n",
    "                    self.causes_mean = torch.tensor(self.causes_mean, device=device).unsqueeze(0).unsqueeze(0).tile(\n",
    "                        (seq_len, 1, 1))\n",
    "                    self.causes_std = torch.tensor(self.causes_std, device=device).unsqueeze(0).unsqueeze(0).tile(\n",
    "                        (seq_len, 1, 1))\n",
    "\n",
    "                def generate_module(layer_idx, out_dim):\n",
    "                    # Determine std of each noise term in initialization, so that is shared in runs\n",
    "                    # torch.abs(torch.normal(torch.zeros((out_dim)), self.noise_std)) - Change std for each dimension?\n",
    "                    noise = (GaussianNoise(torch.abs(torch.normal(torch.zeros(size=(1, out_dim), device=device), float(self.noise_std))), device=device)\n",
    "                         if self.pre_sample_weights else GaussianNoise(float(self.noise_std), device=device))\n",
    "                    return [\n",
    "                        nn.Sequential(*[self.prior_mlp_activations()\n",
    "                            , nn.Linear(self.prior_mlp_hidden_dim, out_dim)\n",
    "                            , noise])\n",
    "                    ]\n",
    "\n",
    "                self.layers = [nn.Linear(self.num_causes, self.prior_mlp_hidden_dim, device=device)]\n",
    "                self.layers += [module for layer_idx in range(self.num_layers-1) for module in generate_module(layer_idx, self.prior_mlp_hidden_dim)]\n",
    "                if not self.is_causal:\n",
    "                    self.layers += generate_module(-1, num_outputs)\n",
    "                self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "                # Initialize Model parameters\n",
    "                for i, (n, p) in enumerate(self.layers.named_parameters()):\n",
    "                    if self.block_wise_dropout:\n",
    "                        if len(p.shape) == 2: # Only apply to weight matrices and not bias\n",
    "                            nn.init.zeros_(p)\n",
    "                            # TODO: N blocks should be a setting\n",
    "                            n_blocks = random.randint(1, math.ceil(math.sqrt(min(p.shape[0], p.shape[1]))))\n",
    "                            w, h = p.shape[0] // n_blocks, p.shape[1] // n_blocks\n",
    "                            keep_prob = (n_blocks*w*h) / p.numel()\n",
    "                            for block in range(0, n_blocks):\n",
    "                                nn.init.normal_(p[w * block: w * (block+1), h * block: h * (block+1)], std=self.init_std / keep_prob**(1/2 if self.prior_mlp_scale_weights_sqrt else 1))\n",
    "                    else:\n",
    "                        if len(p.shape) == 2: # Only apply to weight matrices and not bias\n",
    "                            dropout_prob = self.prior_mlp_dropout_prob if i > 0 else 0.0  # Don't apply dropout in first layer\n",
    "                            dropout_prob = min(dropout_prob, 0.99)\n",
    "                            nn.init.normal_(p, std=self.init_std / (1. - dropout_prob**(1/2 if self.prior_mlp_scale_weights_sqrt else 1)))\n",
    "                            p *= torch.bernoulli(torch.zeros_like(p) + 1. - dropout_prob)\n",
    "\n",
    "        def forward(self):\n",
    "            def sample_normal():\n",
    "                if self.pre_sample_causes:\n",
    "                    causes = torch.normal(self.causes_mean, self.causes_std.abs()).float()\n",
    "                else:\n",
    "                    causes = torch.normal(0., 1., (seq_len, 1, self.num_causes), device=device).float()\n",
    "                return causes\n",
    "\n",
    "            if self.sampling == 'normal':\n",
    "                causes = sample_normal()\n",
    "            elif self.sampling == 'mixed':\n",
    "                zipf_p, multi_p, normal_p = random.random() * 0.66, random.random() * 0.66, random.random() * 0.66\n",
    "                def sample_cause(n):\n",
    "                    if random.random() > normal_p:\n",
    "                        if self.pre_sample_causes:\n",
    "                            return torch.normal(self.causes_mean[:, :, n], self.causes_std[:, :, n].abs()).float()\n",
    "                        else:\n",
    "                            return torch.normal(0., 1., (seq_len, 1), device=device).float()\n",
    "                    elif random.random() > multi_p:\n",
    "                        x = torch.multinomial(torch.rand((random.randint(2, 10))), seq_len, replacement=True).to(device).unsqueeze(-1).float()\n",
    "                        x = (x - torch.mean(x)) / torch.std(x)\n",
    "                        return x\n",
    "                    else:\n",
    "                        x = torch.minimum(torch.tensor(np.random.zipf(2.0 + random.random() * 2, size=(seq_len)),\n",
    "                                            device=device).unsqueeze(-1).float(), torch.tensor(10.0, device=device))\n",
    "                        return x - torch.mean(x)\n",
    "                causes = torch.cat([sample_cause(n).unsqueeze(-1) for n in range(self.num_causes)], -1)\n",
    "            elif self.sampling == 'uniform':\n",
    "                causes = torch.rand((seq_len, 1, self.num_causes), device=device)\n",
    "            else:\n",
    "                raise ValueError(f'Sampling is set to invalid setting: {sampling}.')\n",
    "\n",
    "            outputs = [causes]\n",
    "            for layer in self.layers:\n",
    "                outputs.append(layer(outputs[-1]))\n",
    "            outputs = outputs[2:]\n",
    "\n",
    "            if self.is_causal:\n",
    "                ## Sample nodes from graph if model is causal\n",
    "                outputs_flat = torch.cat(outputs, -1)\n",
    "\n",
    "                if self.in_clique:\n",
    "                    random_perm = random.randint(0, outputs_flat.shape[-1] - num_outputs - num_features) + torch.randperm(num_outputs + num_features, device=device)\n",
    "                else:\n",
    "                    random_perm = torch.randperm(outputs_flat.shape[-1]-1, device=device)\n",
    "\n",
    "                random_idx_y = list(range(-num_outputs, -0)) if self.y_is_effect else random_perm[0:num_outputs]\n",
    "                random_idx = random_perm[num_outputs:num_outputs + num_features]\n",
    "\n",
    "                if self.sort_features:\n",
    "                    random_idx, _ = torch.sort(random_idx)\n",
    "                y = outputs_flat[:, :, random_idx_y]\n",
    "\n",
    "                x = outputs_flat[:, :, random_idx]\n",
    "            else:\n",
    "                y = outputs[-1][:, :, :]\n",
    "                x = causes\n",
    "\n",
    "            if bool(torch.any(torch.isnan(x)).detach().cpu().numpy()) or bool(torch.any(torch.isnan(y)).detach().cpu().numpy()):\n",
    "                print('Nan caught in MLP model x:', torch.isnan(x).sum(), ' y:', torch.isnan(y).sum())\n",
    "                print({k: hyperparameters[k] for k in ['is_causal', 'num_causes', 'prior_mlp_hidden_dim'\n",
    "                    , 'num_layers', 'noise_std', 'y_is_effect', 'pre_sample_weights', 'prior_mlp_dropout_prob'\n",
    "                    , 'pre_sample_causes']})\n",
    "\n",
    "                x[:] = 0.0\n",
    "                y[:] = -100 # default ignore index for CE\n",
    "\n",
    "            # random feature rotation\n",
    "            if self.random_feature_rotation:\n",
    "                x = x[..., (torch.arange(x.shape[-1], device=device)+random.randrange(x.shape[-1])) % x.shape[-1]]\n",
    "\n",
    "            return x, y\n",
    "\n",
    "    if hyperparameters.get('new_mlp_per_example', False):\n",
    "        get_model = lambda: MLP(hyperparameters).to(device)\n",
    "    else:\n",
    "        model = MLP(hyperparameters).to(device)\n",
    "        get_model = lambda: model\n",
    "\n",
    "    sample = [get_model()() for _ in range(0, batch_size)]\n",
    "\n",
    "    x, y = zip(*sample)\n",
    "    y = torch.cat(y, 1).detach().squeeze(2)\n",
    "    x = torch.cat(x, 1).detach()\n",
    "\n",
    "    return x, y, y\n",
    "\n",
    "\n",
    "DataLoader = get_batch_to_dataloader(get_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabpfn/train.py\n",
    "steps_per_epoch = 100 # set to 10\n",
    "batch_size = 200 # set to 1000\n",
    "bptt=10 # default in function train(), not changed afterwards\n",
    "bptt_extra_samples=None # default in function train(), not changed afterwards\n",
    "single_eval_pos_gen=None # default in function train(), not changed afterwards\n",
    "extra_prior_kwargs_dict={} # default in function train(), not changed afterwards\n",
    "gpu_device='cuda:0' # default in function train(), not changed afterwards\n",
    "device = gpu_device if torch.cuda.is_available() else 'cpu:0'\n",
    "single_eval_pos_gen=None\n",
    "\n",
    "def eval_pos_seq_len_sampler():\n",
    "    single_eval_pos = single_eval_pos_gen()\n",
    "    if bptt_extra_samples:\n",
    "        return single_eval_pos, single_eval_pos + bptt_extra_samples\n",
    "    else:\n",
    "        return single_eval_pos, bptt\n",
    "\n",
    "priordataloader_class = DataLoader\n",
    "\n",
    "dl = priordataloader_class(num_steps=steps_per_epoch, batch_size=batch_size, eval_pos_seq_len_sampler=eval_pos_seq_len_sampler, seq_len_maximum=bptt+(bptt_extra_samples if bptt_extra_samples else 0), device=device, **extra_prior_kwargs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(self): # does not increase epoch_count\n",
    "            return self.gbm(**self.get_batch_kwargs, epoch=self.epoch_count, model=self.model if hasattr(self, 'model') else None)\n",
    "\n",
    "def get_uniform_single_eval_pos_sampler(max_len, min_len=0):\n",
    "    \"\"\"\n",
    "    Just sample any evaluation position with the same weight\n",
    "    :return: Sampler that can be fed to `train()` as `single_eval_pos_gen`.\n",
    "    \"\"\"\n",
    "    return lambda: random.choices(range(min_len, max_len))[0]\n",
    "\n",
    "get_sampler = get_uniform_single_eval_pos_sampler\n",
    "permutation_invariant_max_eval_pos = 100 # very random, had to set it to sth but don't know what this is\n",
    "\n",
    "single_eval_pos_gen = get_sampler(permutation_invariant_max_eval_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_batch() missing 2 required positional arguments: 'num_features' and 'hyperparameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e20f57b4cebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstyle_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the style in batch of the form ((style, x, y), target, single_eval_pos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-44b2f61d778c>\u001b[0m in \u001b[0;36mget_test_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_test_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# does not increase epoch_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-44b2f61d778c>\u001b[0m in \u001b[0;36mgbm\u001b[0;34m(eval_pos_seq_len_sampler, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'dynamic_batch_size'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dynamic_batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dynamic_batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_len_maximum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dynamic_batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dynamic_batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_method_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'single_eval_pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_batch() missing 2 required positional arguments: 'num_features' and 'hyperparameters'"
     ]
    }
   ],
   "source": [
    "style_def = dl.get_test_batch()[0][0] # the style in batch of the form ((style, x, y), target, single_eval_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e0db07081617c9a6eca3dcd9ae8397d12b2119e3e97abe44125c2c5f990a5fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
