architectures: 
  - ElectraForPreTraining
attention_probs_dropout_prob: 0.1
embedding_size: 1024
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 1024
initializer_range: 0.02
intermediate_size: 4096
layer_norm_eps: 1e-12
max_position_embeddings: ${oc.select:task.train.convert_electra.max_seq_length,512}
model_type: electra
num_attention_heads: 16
num_hidden_layers: 24
pad_token_id: 0
type_vocab_size: 2
vocab_size: ${oc.select:task.train.convert_electra.vocab_size,32000}