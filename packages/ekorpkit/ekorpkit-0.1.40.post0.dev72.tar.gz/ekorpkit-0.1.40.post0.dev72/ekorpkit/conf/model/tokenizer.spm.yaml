defaults:
  - tokenizer
# input:
# one-sentence-per-line raw corpus file.
# No need to run tokenizer, normalizer or preprocessor.
# By default, SentencePiece normalizes the input with Unicode NFKC.
# You can pass a comma-separated list of files.
vocab_size: 30000
model_type: unigram
# model type. Choose from unigram (default), bpe, char, or word.
# The input sentence must be pretokenized when using word type.
character_coverage: 1.0
# amount of characters covered by the model,
# good defaults are: 0.9995 for languages with rich character set like Japanese or Chinese
# and 1.0 for other languages with small character set.
num_threads: ${oc.select:..batch.num_workers, 1}
train_extremely_large_corpus: false
# If true, train the model using the --input_sentence_size option.
# This option is useful when the input corpus is extremely large.
# The input_sentence_size is set to 100M by default.
# If you want to change the value, set the --input_sentence_size option.
# For example, --input_sentence_size=1000000000 means 1 billion.
# The input_sentence_size must be larger than 1M.
# control_symbols: ""
# input_sentence_size:
# shuffle_input_sentence: true
# add_dummy_prefix: false
# "pad_id": 0,
# "unk_id": 1,
# "bos_id": 2,
# "eos_id": 3,
# "pad_piece":
# "unk_piece":
# "bos_piece":
# "eos_piece":
# "user_defined_symbols":
