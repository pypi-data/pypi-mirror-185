# FORMAT
# Put your extra requirements here in the following format
#
# package[version_required]: tag1, tag2, ...
# ex) pip install -e ."[modin]"

fugashi: all, tokenize, fugashi
mecab-python3: all, tokenize, mecab
mecab-ko-dic: all, tokenize, mecab
pysbd: all, tokenize, dataset, tokenize-en
pynori: all, tokenize, pynori
soynlp: all, tokenize, soynlp
nltk: all, tokenize, nltk, tokenize-en
sacremoses: all, tokenize
emoji<2.0: all, tokenize, dataset
spacy: tokenize
sentencepiece: tokenize
tokenizers: tokenize

tomotopy: all, topic
wordcloud: all, topic, visualize
pyLDAvis: all, topic

pyhwp: parser, hwp, bok
wikiextractor: parser, wiki
namu-wiki-extractor: parser, wiki
pdfplumber: parser, fomc
mail-parser: parser, mail
pubmed_parser: parser, pubmed
jsonpath-ng: all, parser, dataset
beautifulsoup4: all, parser, dataset
html-to-json: all, parser, html
fasttext-langdetect: parser, langdetect
cssutils: all, parser, edgar

evaluate: all, transformers, model
simpletransformers>=0.63: all, transformers, model
datasets: all, transformers, dataset, model, art, tokenize
# torch: transformers
tensorflow: all, model
# tf-nightly: transformers, model
flaml: automl, model, all
yellowbrick: yellowbrick
scipy: automl, model, all
wandb: all, model, art
GPUtil: all, model, art

boto3>=1.0,<2.0: aws, cached-path
google-api-python-client: google
google-cloud-storage>=1.32.0,<3.0: google, cached-path
huggingface-hub>=0.8.1,<0.9.0: hf, cached-path

modin: ddbackend
ray: ddbackend
# dask: pandas
# swifter: pandas

# sysrsync: fetch
wget: all, fetch, dataset
pycurl: all, fetch, dataset
py7zr: all, fetch, dataset
zstandard: all, fetch, dataset, tokenize
# cached-path: fetch, dataset

pymongo: database

# py-markdown-table: doc
# pytablewriter: doc
plotly: all, doc, visualize
kaleido: all, doc, visualize
rich: all, doc, visualize
matplotlib: all, doc, visualize, art
seaborn: all, doc, visualize
Pillow<9.1.0: all, doc, visualize, art

tensorflow-datasets: dataset
# tensorflow-io: dataset
lsh: all, dataset
orjson: all, dataset, parser, fetch, tokenize
datasketch: all, dataset
pysimdjson: all, dataset, parser
p_tqdm: all, dataset, parser
joblib: all, dataset, parser
# ftfy: all, dataset, parser, tokenize, disco
jsonlines: all, dataset, parser
loky: all, dataset, parser
pathos: all, dataset, parser, edgar
guesslang: guesslang
openpyxl: all, dataset, parser

numba: numba

fredapi: all, fred, fomc
nasdaq-data-link: all, nasdaq, fomc
statsmodels: all, fomc, stats

lpips: disco, art
timm: disco, art
einops: disco, art
jax: dalle-mini, art
flax: art, dalle-mini
unidecode: art, disco
opencv-python==4.5.5.64: art, disco
diffusers: art, stable, all

cleanlab: label, all
# snorkel: label
argilla: label

dicttoxml: all, xml
